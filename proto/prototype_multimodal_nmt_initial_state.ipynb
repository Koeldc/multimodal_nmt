{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "from theano import tensor\n",
    "from toolz import merge\n",
    "import os\n",
    "\n",
    "import numpy\n",
    "\n",
    "from fuel.datasets import IterableDataset\n",
    "from fuel.transformers import Merge\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from blocks.bricks import (Tanh, Maxout, Linear, FeedforwardSequence,\n",
    "                           Bias, Initializable, MLP)\n",
    "from blocks.bricks.attention import SequenceContentAttention\n",
    "from blocks.bricks.base import application\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Bidirectional\n",
    "from blocks.bricks.sequence_generators import (\n",
    "    LookupFeedback, Readout, SoftmaxEmitter,\n",
    "    SequenceGenerator)\n",
    "from blocks.roles import add_role, WEIGHT\n",
    "from blocks.utils import shared_floatx_nans\n",
    "\n",
    "from machine_translation.models import MinRiskSequenceGenerator\n",
    "\n",
    "from picklable_itertools.extras import equizip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an NMT decoder which has access to image features via the target-side initial state\n",
    "\n",
    "# IDEA: subclass attention recurrent, and add one more context\n",
    "# -- could directly push the context onto attention_recurrent.context_names?\n",
    "\n",
    "# it's ok to add directly to the contexts of the recurrent transition, since that's what will be using them anyway,\n",
    "# TEST 1: what happens when we directly add the image features to the kwargs that we pass to sequence_generator.cost?\n",
    "# note this is similar to IMT, since we're trying to modify the decoder initial state\n",
    "\n",
    "# The kwargs do get passed through to the recurrent transition, so this should work\n",
    "\n",
    "# AttentionRecurrent gets created in the SequenceGenerator init(), which then calls BaseSequenceGenerator\n",
    "# Subclass SequenceGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add one more source for the images\n",
    "\n",
    "# get the MT datastream in the standard way, then add the new source using Merge\n",
    "# -- the problem with this is all the operations we do on the stream beforehand\n",
    "\n",
    "# as long as the arrays fit in memory, we should be able to use iterable dataset\n",
    "\n",
    "TRAIN_IMAGE_FEATURES = '/media/1tb_drive/multilingual-multimodal/flickr30k/img_features/f30k-translational-newsplits/train.npz'\n",
    "DEV_IMAGE_FEATURES = '/media/1tb_drive/multilingual-multimodal/flickr30k/img_features/f30k-translational-newsplits/dev.npz'\n",
    "TEST_IMAGE_FEATURES = '/media/1tb_drive/multilingual-multimodal/flickr30k/img_features/f30k-translational-newsplits/test.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the prototype config for NMT experiment with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BASEDIR = '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "          '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/'\n",
    "#best_bleu_model_1455464992_BLEU31.61.npz\n",
    "\n",
    "exp_config = {\n",
    "    'src_vocab_size': 20000,\n",
    "    'trg_vocab_size': 20000,\n",
    "    'enc_embed': 300,\n",
    "    'dec_embed': 300,\n",
    "    'enc_nhids': 800,\n",
    "    'dec_nhids': 800,\n",
    "    'src_vocab': os.path.join(BASEDIR, 'vocab.en-de.en.pkl'),\n",
    "    'trg_vocab': os.path.join(BASEDIR, 'vocab.en-de.de.pkl'),\n",
    "    'src_data': os.path.join(BASEDIR, 'training_data/train.en.tok.shuf'),\n",
    "    'trg_data': os.path.join(BASEDIR, 'training_data/train.de.tok.shuf'),\n",
    "    'unk_id':1,\n",
    "    # Bleu script that will be used (moses multi-perl in this case)\n",
    "    'bleu_script': '/home/chris/projects/neural_mt/test_data/sample_experiment/tiny_demo_dataset/multi-bleu.perl',\n",
    "\n",
    "    # Optimization related ----------------------------------------------------\n",
    "    # Batch size\n",
    "    'batch_size': 8,\n",
    "    # This many batches will be read ahead and sorted\n",
    "    'sort_k_batches': 2,\n",
    "    # Optimization step rule\n",
    "    'step_rule': 'AdaDelta',\n",
    "    # Gradient clipping threshold\n",
    "    'step_clipping': 1.,\n",
    "    # Std of weight initialization\n",
    "    'weight_scale': 0.01,\n",
    "    'seq_len': 40,\n",
    "    # Beam-size\n",
    "    'beam_size': 10,\n",
    "\n",
    "    # Maximum number of updates\n",
    "    'finish_after': 1000000,\n",
    "\n",
    "    # Reload model from files if exist\n",
    "    'reload': False,\n",
    "\n",
    "    # Save model after this many updates\n",
    "    'save_freq': 500,\n",
    "\n",
    "    # Show samples from model after this many updates\n",
    "    'sampling_freq': 1000,\n",
    "\n",
    "    # Show this many samples at each sampling\n",
    "    'hook_samples': 5,\n",
    "\n",
    "    # Validate bleu after this many updates\n",
    "    'bleu_val_freq': 10,\n",
    "    # Normalize cost according to sequence length after beam-search\n",
    "    'normalized_bleu': True,\n",
    "    \n",
    "    'saveto': '/media/1tb_drive/test_min_risk_model_save',\n",
    "    'model_save_directory': 'test_image_context_features_model_save',\n",
    "    \n",
    "    # Validation set source file\n",
    "    'val_set': '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/dev.en.tok',\n",
    "\n",
    "    # Validation set gold file\n",
    "    'val_set_grndtruth': '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/dev.de.tok',\n",
    "\n",
    "    # Print validation output to file\n",
    "    'output_val_set': True,\n",
    "\n",
    "    # Validation output file\n",
    "    'val_set_out': '/media/1tb_drive/test_min_risk_model_save/validation_out.txt',\n",
    "    'val_burn_in': 0,\n",
    "\n",
    "    #     'saved_parameters': '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/BERTHA-TEST_wmt-multimodal_internal_data_dropout0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/best_bleu_model_1455410311_BLEU30.38.npz',\n",
    "\n",
    "    # NEW PARAMS FOR ADDING CONTEXT FEATURES\n",
    "    'context_features': '/media/1tb_drive/multilingual-multimodal/flickr30k/img_features/f30k-translational-newsplits/train.npz', \n",
    "    'val_context_features': '/media/1tb_drive/multilingual-multimodal/flickr30k/img_features/f30k-translational-newsplits/dev.npz'\n",
    "    \n",
    "    # NEW PARAM FOR MIN RISK\n",
    "#     'n_samples': 100\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from machine_translation.stream import _ensure_special_tokens, _length, PaddingWithEOS, _oov_to_unk, _too_long\n",
    "\n",
    "def get_tr_stream_with_context_features(src_vocab, trg_vocab, src_data, trg_data, context_features,\n",
    "                  src_vocab_size=30000, trg_vocab_size=30000, unk_id=1,\n",
    "                  seq_len=50, batch_size=80, sort_k_batches=12, **kwargs):\n",
    "    \"\"\"Prepares the training data stream.\"\"\"\n",
    "\n",
    "    def _get_np_array(filename):\n",
    "        return numpy.load(filename)['arr_0']\n",
    "    \n",
    "    # Load dictionaries and ensure special tokens exist\n",
    "    src_vocab = _ensure_special_tokens(\n",
    "        src_vocab if isinstance(src_vocab, dict)\n",
    "        else cPickle.load(open(src_vocab)),\n",
    "        bos_idx=0, eos_idx=src_vocab_size - 1, unk_idx=unk_id)\n",
    "    trg_vocab = _ensure_special_tokens(\n",
    "        trg_vocab if isinstance(trg_vocab, dict) else\n",
    "        cPickle.load(open(trg_vocab)),\n",
    "        bos_idx=0, eos_idx=trg_vocab_size - 1, unk_idx=unk_id)\n",
    "\n",
    "    # Get text files from both source and target\n",
    "    src_dataset = TextFile([src_data], src_vocab, None)\n",
    "    trg_dataset = TextFile([trg_data], trg_vocab, None)\n",
    "\n",
    "    # Merge them to get a source, target pair\n",
    "    stream = Merge([src_dataset.get_example_stream(),\n",
    "                    trg_dataset.get_example_stream()],\n",
    "                   ('source', 'target'))\n",
    "\n",
    "    # Filter sequences that are too long\n",
    "    stream = Filter(stream,\n",
    "                    predicate=_too_long(seq_len=seq_len))\n",
    "    \n",
    "  \n",
    "    # Replace out of vocabulary tokens with unk token\n",
    "    # TODO: doesn't the TextFile stream do this anyway?\n",
    "    stream = Mapping(stream,\n",
    "                     _oov_to_unk(src_vocab_size=src_vocab_size,\n",
    "                                 trg_vocab_size=trg_vocab_size,\n",
    "                                 unk_id=unk_id))\n",
    "\n",
    "    # now add the source with the image features\n",
    "    # create the image datastream (iterate over a file line-by-line)\n",
    "    train_features = _get_np_array(context_features)\n",
    "    train_feature_dataset = IterableDataset(train_features)\n",
    "    train_image_stream = DataStream(train_feature_dataset)\n",
    "\n",
    "    stream = Merge([stream, train_image_stream], ('source', 'target', 'initial_contexts'))\n",
    "    \n",
    "    # Build a batched version of stream to read k batches ahead\n",
    "    stream = Batch(stream,\n",
    "                   iteration_scheme=ConstantScheme(\n",
    "                       batch_size*sort_k_batches))\n",
    "\n",
    "    # Sort all samples in the read-ahead batch\n",
    "    stream = Mapping(stream, SortMapping(_length))\n",
    "\n",
    "    # Convert it into a stream again\n",
    "    stream = Unpack(stream)\n",
    "\n",
    "    # Construct batches from the stream with specified batch size\n",
    "    stream = Batch(\n",
    "        stream, iteration_scheme=ConstantScheme(batch_size))\n",
    "\n",
    "    # Pad sequences that are short\n",
    "    masked_stream = PaddingWithEOS(\n",
    "        stream, [src_vocab_size - 1, trg_vocab_size - 1], mask_sources=('source', 'target'))\n",
    "\n",
    "    return masked_stream\n",
    "\n",
    "\n",
    "# Remember that the BleuValidator does hackish stuff to get target set information from the main_loop data_stream\n",
    "# using all kwargs here makes it more clear that this function is always called with get_dev_stream(**config_dict)\n",
    "def get_dev_stream_with_context_features(val_context_features=None, val_set=None, src_vocab=None,\n",
    "                                         src_vocab_size=30000, unk_id=1, **kwargs):\n",
    "    \"\"\"Setup development set stream if necessary.\"\"\"\n",
    "    dev_stream = None\n",
    "    if val_set is not None and src_vocab is not None:\n",
    "        src_vocab = _ensure_special_tokens(\n",
    "            src_vocab if isinstance(src_vocab, dict) else\n",
    "            cPickle.load(open(src_vocab)),\n",
    "            bos_idx=0, eos_idx=src_vocab_size - 1, unk_idx=unk_id)\n",
    "        \n",
    "        # TODO: how is the dev dataset used without the context features?\n",
    "        dev_dataset = TextFile([val_set], src_vocab, None)\n",
    "        \n",
    "        dev_stream = DataStream(dev_dataset)\n",
    "    return dev_stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WORKING: how do BLEU validator and beam search need to be modified to account for the new context? \n",
    "# TODO: How does sampling change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Running Neural Machine Translation in mode: train\n"
     ]
    }
   ],
   "source": [
    "# setting up the experiment\n",
    "    \n",
    "# args = parser.parse_args()\n",
    "# arg_dict = vars(args)\n",
    "# configuration_file = arg_dict['exp_config']\n",
    "# mode = arg_dict['mode']\n",
    "\n",
    "mode = 'train'\n",
    "logger.info('Running Neural Machine Translation in mode: {}'.format(mode))\n",
    "# config_obj = configurations.get_config(configuration_file)\n",
    "config_obj = exp_config\n",
    "\n",
    "# add the config file name into config_obj\n",
    "# config_obj['config_file'] = configuration_file\n",
    "# logger.info(\"Model Configuration:\\n{}\".format(pprint.pformat(config_obj)))\n",
    "\n",
    "train_stream = get_tr_stream_with_context_features(**config_obj)\n",
    "dev_stream = get_dev_stream_with_context_features(**config_obj)\n",
    "\n",
    "\n",
    "# bokeh = True\n",
    "\n",
    "# if mode == 'train':\n",
    "    # Get data streams and call main\n",
    "#     main(config_obj, get_tr_stream(**config_obj),\n",
    "#          get_dev_stream(**config_obj), bokeh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = next(train_stream.get_epoch_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 16), (8, 16), (8, 13), (8, 13), (8, 4096)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('source', 'source_mask', 'target', 'target_mask', 'initial_contexts')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stream.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRUInitialStateWithInitialStateContext(GatedRecurrent):\n",
    "    \"\"\"Gated Recurrent with special initial state.\n",
    "\n",
    "    Initial state of Gated Recurrent is set by an MLP that conditions on the\n",
    "    last hidden state of the bidirectional encoder, applies an affine\n",
    "    transformation followed by a tanh non-linearity to set initial state.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, attended_dim, context_dim, **kwargs):\n",
    "        super(GRUInitialState, self).__init__(**kwargs)\n",
    "        self.attended_dim = attended_dim\n",
    "        self.attended_dim = attended_dim\n",
    "        self.context_dim = context_dim\n",
    "\n",
    "        self.initial_transformer = MLP(activations=[Tanh(),Tanh(),Tanh()],\n",
    "                                       dims=[attended_dim + context_dim, 1000, 500, self.dim],\n",
    "                                       name='state_initializer')\n",
    "        self.children.append(self.initial_transformer)\n",
    "  \n",
    "    # WORKING: add the images as another context to the recurrent transition\n",
    "    # THINKING: how to best combine the image info with the source info?\n",
    "    @application\n",
    "    def initial_states(self, batch_size, *args, **kwargs):\n",
    "        attended = kwargs['attended']\n",
    "        context = kwargs['initial_state_context']\n",
    "        attended_reverse_final_state = attended[0, :, -self.attended_dim:]\n",
    "        concat_attended_and_context = tensor.concatenate([attended_reverse_final_state, context], axis=1)\n",
    "        initial_state = self.initial_transformer.apply(\n",
    "            attended[0, :, -self.attended_dim:])\n",
    "        return initial_state\n",
    "\n",
    "    def _allocate(self):\n",
    "        self.parameters.append(shared_floatx_nans((self.dim, self.dim),\n",
    "                               name='state_to_state'))\n",
    "        self.parameters.append(shared_floatx_nans((self.dim, 2 * self.dim),\n",
    "                               name='state_to_gates'))\n",
    "        for i in range(2):\n",
    "            if self.parameters[i]:\n",
    "                add_role(self.parameters[i], WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InitialContextDecoder(Initializable):\n",
    "    \"\"\"\n",
    "    Decoder which incorporates context features into the target-side initial state\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    vocab_size: int\n",
    "    embedding_dim: int\n",
    "    representation_dim: int\n",
    "    theano_seed: int\n",
    "    loss_function: str : {'cross_entropy'(default) | 'min_risk'}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(*args, **kwargs):\n",
    "        super(InitialContextDecoder, self).__init__(*args, **kwargs)   \n",
    "\n",
    "    @application(inputs=['representation', 'source_sentence_mask',\n",
    "                         'target_sentence_mask', 'target_sentence', 'initial_state_context'],\n",
    "                 outputs=['cost'])\n",
    "    def cost(self, representation, source_sentence_mask,\n",
    "             target_sentence, target_sentence_mask, initial_state_context):\n",
    "\n",
    "        source_sentence_mask = source_sentence_mask.T\n",
    "        target_sentence = target_sentence.T\n",
    "        target_sentence_mask = target_sentence_mask.T\n",
    "\n",
    "        # Get the cost matrix\n",
    "        cost = self.sequence_generator.cost_matrix(**{\n",
    "            'mask': target_sentence_mask,\n",
    "            'outputs': target_sentence,\n",
    "            'attended': representation,\n",
    "            'attended_mask': source_sentence_mask,\n",
    "            'initial_state_context': initial_state_context}\n",
    "        )\n",
    "\n",
    "        return (cost * target_sentence_mask).sum() / \\\n",
    "            target_sentence_mask.shape[1]\n",
    "\n",
    "    # Note: this requires the decoder to be using sequence_generator which implements expected cost\n",
    "#     @application(inputs=['representation', 'source_sentence_mask',\n",
    "#                          'target_samples_mask', 'target_samples', 'scores'],\n",
    "#                  outputs=['cost'])\n",
    "#     def expected_cost(self, representation, source_sentence_mask, target_samples, target_samples_mask, scores,\n",
    "#                       **kwargs):\n",
    "#         return self.sequence_generator.expected_cost(representation,\n",
    "#                                                      source_sentence_mask,\n",
    "#                                                      target_samples, target_samples_mask, scores, **kwargs)\n",
    "\n",
    "\n",
    "#     @application\n",
    "#     def generate(self, source_sentence, representation, **kwargs):\n",
    "#         return self.sequence_generator.generate(\n",
    "#             n_steps=2 * source_sentence.shape[1],\n",
    "#             batch_size=source_sentence.shape[0],\n",
    "#             attended=representation,\n",
    "#             attended_mask=tensor.ones(source_sentence.shape).T,\n",
    "#             **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from theano import tensor\n",
    "from toolz import merge\n",
    "import numpy\n",
    "import pickle\n",
    "from subprocess import Popen, PIPE\n",
    "import codecs\n",
    "\n",
    "from blocks.algorithms import (GradientDescent, StepClipping,\n",
    "                               CompositeRule, Adam, AdaDelta)\n",
    "from blocks.extensions import FinishAfter, Printing, Timing\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.graph import ComputationGraph, apply_noise, apply_dropout\n",
    "from blocks.initialization import IsotropicGaussian, Orthogonal, Constant\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.select import Selector\n",
    "from blocks.search import BeamSearch\n",
    "from blocks_extras.extensions.plot import Plot\n",
    "\n",
    "from machine_translation.checkpoint import CheckpointNMT, LoadNMT\n",
    "from machine_translation.model import BidirectionalEncoder, Decoder\n",
    "from machine_translation.sampling import BleuValidator, Sampler, SamplingBase\n",
    "from machine_translation.stream import (get_tr_stream, get_dev_stream,\n",
    "                                        _ensure_special_tokens)\n",
    "\n",
    "try:\n",
    "    from blocks_extras.extensions.plot import Plot\n",
    "    BOKEH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    BOKEH_AVAILABLE = False\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def main(config, tr_stream, dev_stream, use_bokeh=False):\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    source_sentence = tensor.lmatrix('source')\n",
    "    source_sentence_mask = tensor.matrix('source_mask')\n",
    "    target_sentence = tensor.lmatrix('target')\n",
    "    target_sentence_mask = tensor.matrix('target_mask')\n",
    "    initial_context = tensor.matrix('initial_context')\n",
    "    \n",
    "    sampling_input = tensor.lmatrix('input')\n",
    "\n",
    "    # Construct model\n",
    "    logger.info('Building RNN encoder-decoder')\n",
    "    encoder = BidirectionalEncoder(\n",
    "        config['src_vocab_size'], config['enc_embed'], config['enc_nhids'])\n",
    "    decoder = Decoder(\n",
    "        config['trg_vocab_size'], config['dec_embed'], config['dec_nhids'],\n",
    "        config['enc_nhids'] * 2)\n",
    "    cost = decoder.cost(\n",
    "        encoder.apply(source_sentence, source_sentence_mask),\n",
    "        source_sentence_mask, target_sentence, target_sentence_mask)\n",
    "\n",
    "    logger.info('Creating computational graph')\n",
    "    cg = ComputationGraph(cost)\n",
    "\n",
    "    # Initialize model\n",
    "    logger.info('Initializing model')\n",
    "    encoder.weights_init = decoder.weights_init = IsotropicGaussian(\n",
    "        config['weight_scale'])\n",
    "    encoder.biases_init = decoder.biases_init = Constant(0)\n",
    "    encoder.push_initialization_config()\n",
    "    decoder.push_initialization_config()\n",
    "    encoder.bidir.prototype.weights_init = Orthogonal()\n",
    "    decoder.transition.weights_init = Orthogonal()\n",
    "    encoder.initialize()\n",
    "    decoder.initialize()\n",
    "\n",
    "    # apply dropout for regularization\n",
    "    if config['dropout'] < 1.0:\n",
    "        # dropout is applied to the output of maxout in ghog\n",
    "        # this is the probability of dropping out, so you probably want to make it <=0.5\n",
    "        logger.info('Applying dropout')\n",
    "        dropout_inputs = [x for x in cg.intermediary_variables\n",
    "                          if x.name == 'maxout_apply_output']\n",
    "        cg = apply_dropout(cg, dropout_inputs, config['dropout'])\n",
    "\n",
    "    # Apply weight noise for regularization\n",
    "    if config['weight_noise_ff'] > 0.0:\n",
    "        logger.info('Applying weight noise to ff layers')\n",
    "        enc_params = Selector(encoder.lookup).get_parameters().values()\n",
    "        enc_params += Selector(encoder.fwd_fork).get_parameters().values()\n",
    "        enc_params += Selector(encoder.back_fork).get_parameters().values()\n",
    "        dec_params = Selector(\n",
    "            decoder.sequence_generator.readout).get_parameters().values()\n",
    "        dec_params += Selector(\n",
    "            decoder.sequence_generator.fork).get_parameters().values()\n",
    "        dec_params += Selector(decoder.transition.initial_transformer).get_parameters().values()\n",
    "        cg = apply_noise(cg, enc_params+dec_params, config['weight_noise_ff'])\n",
    "\n",
    "    # TODO: weight noise for recurrent params isn't currently implemented -- see config['weight_noise_rec']\n",
    "    # Print shapes\n",
    "    shapes = [param.get_value().shape for param in cg.parameters]\n",
    "    logger.info(\"Parameter shapes: \")\n",
    "    for shape, count in Counter(shapes).most_common():\n",
    "        logger.info('    {:15}: {}'.format(shape, count))\n",
    "    logger.info(\"Total number of parameters: {}\".format(len(shapes)))\n",
    "\n",
    "    # Print parameter names\n",
    "    enc_dec_param_dict = merge(Selector(encoder).get_parameters(),\n",
    "                               Selector(decoder).get_parameters())\n",
    "    logger.info(\"Parameter names: \")\n",
    "    for name, value in enc_dec_param_dict.items():\n",
    "        logger.info('    {:15}: {}'.format(value.get_value().shape, name))\n",
    "    logger.info(\"Total number of parameters: {}\"\n",
    "                .format(len(enc_dec_param_dict)))\n",
    "\n",
    "    # Set up training model\n",
    "    logger.info(\"Building model\")\n",
    "    training_model = Model(cost)\n",
    "\n",
    "    # create the training directory, and copy this config there if directory doesn't exist\n",
    "    if not os.path.isdir(config['saveto']):\n",
    "        os.makedirs(config['saveto'])\n",
    "        shutil.copy(config['config_file'], config['saveto'])\n",
    "\n",
    "    # Set extensions\n",
    "    logger.info(\"Initializing extensions\")\n",
    "    extensions = [\n",
    "        FinishAfter(after_n_batches=config['finish_after']),\n",
    "        TrainingDataMonitoring([cost], after_batch=True),\n",
    "        Printing(after_batch=True),\n",
    "        CheckpointNMT(config['saveto'],\n",
    "                      every_n_batches=config['save_freq'])\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Set up beam search and sampling computation graphs if necessary\n",
    "\n",
    "    if config['hook_samples'] >= 1 or config['bleu_script'] is not None:\n",
    "        logger.info(\"Building sampling model\")\n",
    "        sampling_representation = encoder.apply(\n",
    "            sampling_input, tensor.ones(sampling_input.shape))\n",
    "        # TODO: the generated output actually contains several more values, ipdb to see what they are\n",
    "        generated = decoder.generate(sampling_input, sampling_representation)\n",
    "        search_model = Model(generated)\n",
    "        _, samples = VariableFilter(\n",
    "            bricks=[decoder.sequence_generator], name=\"outputs\")(\n",
    "                ComputationGraph(generated[1]))  # generated[1] is next_outputs\n",
    "\n",
    "    # Add sampling\n",
    "    if config['hook_samples'] >= 1:\n",
    "        logger.info(\"Building sampler\")\n",
    "        extensions.append(\n",
    "            Sampler(model=search_model, data_stream=tr_stream,\n",
    "                    hook_samples=config['hook_samples'],\n",
    "                    every_n_batches=config['sampling_freq'],\n",
    "                    src_vocab_size=config['src_vocab_size']))\n",
    "\n",
    "    # Add early stopping based on bleu\n",
    "    if config['bleu_script'] is not None:\n",
    "        logger.info(\"Building bleu validator\")\n",
    "        extensions.append(\n",
    "            BleuValidator(sampling_input, samples=samples, config=config,\n",
    "                          model=search_model, data_stream=dev_stream,\n",
    "                          normalize=config['normalized_bleu'],\n",
    "                          every_n_batches=config['bleu_val_freq']))\n",
    "\n",
    "    # Reload model if necessary\n",
    "    if config['reload']:\n",
    "        extensions.append(LoadNMT(config['saveto']))\n",
    "\n",
    "    # Plot cost in bokeh if necessary\n",
    "    if use_bokeh and BOKEH_AVAILABLE:\n",
    "        extensions.append(\n",
    "            Plot(config['model_save_directory'], channels=[['decoder_cost_cost'], ['validation_set_bleu_score']],\n",
    "                 every_n_batches=10))\n",
    "\n",
    "    # Set up training algorithm\n",
    "    logger.info(\"Initializing training algorithm\")\n",
    "    # if there is dropout or random noise, we need to use the output of the modified graph\n",
    "    if config['dropout'] < 1.0 or config['weight_noise_ff'] > 0.0:\n",
    "        algorithm = GradientDescent(\n",
    "            cost=cg.outputs[0], parameters=cg.parameters,\n",
    "            step_rule=CompositeRule([StepClipping(config['step_clipping']),\n",
    "                                     eval(config['step_rule'])()])\n",
    "        )\n",
    "    else:\n",
    "        algorithm = GradientDescent(\n",
    "            cost=cost, parameters=cg.parameters,\n",
    "            step_rule=CompositeRule([StepClipping(config['step_clipping']),\n",
    "                                     eval(config['step_rule'])()])\n",
    "        )\n",
    "\n",
    "    # enrich the logged information\n",
    "    extensions.append(\n",
    "        Timing(every_n_batches=100)\n",
    "    )\n",
    "\n",
    "    # Initialize main loop\n",
    "    logger.info(\"Initializing main loop\")\n",
    "    main_loop = MainLoop(\n",
    "        model=training_model,\n",
    "        algorithm=algorithm,\n",
    "        data_stream=tr_stream,\n",
    "        extensions=extensions\n",
    "    )\n",
    "\n",
    "    # Train!\n",
    "    main_loop.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(encoder, decoder):\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    source_sentence = tensor.lmatrix('source')\n",
    "    source_sentence_mask = tensor.matrix('source_mask')\n",
    "    \n",
    "    \n",
    "\n",
    "    initial_context = tensor.matrix('initial_context')\n",
    "    \n",
    "    representation, source_sentence_mask,\n",
    "             target_sentence, target_sentence_mask, initial_state_context\n",
    "\n",
    "    # the name is important to make sure pre-trained params get loaded correctly\n",
    "    # decoder.name = 'decoder'\n",
    "\n",
    "    # This is the part that is different for the MinimumRiskSequenceGenerator\n",
    "    cost = initial_context_decoder.cost(\n",
    "        encoder.apply(source_sentence, source_sentence_mask),\n",
    "        source_sentence_mask, samples, samples_mask, scores)\n",
    "\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = next(train_image_stream.get_epoch_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x[0])\n",
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "theano_sample_func = sample_model.get_theano_function()\n",
    "\n",
    "# close over the sampling func and the trg_vocab to standardize the interface\n",
    "# TODO: actually this should be a callable class with params (sampling_func, trg_vocab)\n",
    "# TODO: we may be able to make this function faster by passing multiple sources for sampling at the same damn time\n",
    "# TODO: or by avoiding the for loop somehow\n",
    "def sampling_func(source_seq, num_samples=1):\n",
    "\n",
    "    def _get_true_length(seqs, vocab):\n",
    "        try:\n",
    "            lens = []\n",
    "            for r in seqs.tolist():\n",
    "                lens.append(r.index(vocab['</S>']) + 1)\n",
    "            return lens\n",
    "        except ValueError:\n",
    "            return [seqs.shape[1] for _ in range(seqs.shape[0])]\n",
    "\n",
    "    # samples = []\n",
    "    # for _ in range(num_samples):\n",
    "        # outputs of self.sampling_fn = outputs of sequence_generator.generate: next_states + [next_outputs] +\n",
    "        #                 list(next_glimpses.values()) + [next_costs])\n",
    "        # _1, outputs, _2, _3, costs = theano_sample_func(source_seq[None, :])\n",
    "        # if we are generating a single sample, the length of the output will be len(source_seq)*2\n",
    "        # see decoder.generate\n",
    "        # the output is a [seq_len, 1] array\n",
    "        # outputs = outputs.reshape(outputs.shape[0])\n",
    "        # outputs = outputs[:_get_true_length(outputs, trg_vocab)]\n",
    "        # samples.append(outputs)\n",
    "\n",
    "    inputs = numpy.tile(source_seq[None, :], (num_samples, 1))\n",
    "    # the output is [seq_len, batch]\n",
    "    _1, outputs, _2, _3, costs = theano_sample_func(inputs)\n",
    "    outputs = outputs.T\n",
    "\n",
    "    # TODO: this step could be avoided by computing the samples mask in a different way\n",
    "    lens = _get_true_length(outputs, trg_vocab)\n",
    "    samples = [s[:l] for s,l in zip(outputs.tolist(), lens)]\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "src_stream = get_textfile_stream(source_file=exp_config['src_data'], src_vocab=exp_config['src_vocab'],\n",
    "                                         src_vocab_size=exp_config['src_vocab_size'])\n",
    "\n",
    "# test_source_stream.sources = ('sources',)\n",
    "trg_stream = get_textfile_stream(source_file=exp_config['trg_data'], src_vocab=exp_config['trg_vocab'],\n",
    "                                         src_vocab_size=exp_config['trg_vocab_size'])\n",
    "\n",
    "# Merge them to get a source, target pair\n",
    "training_stream = Merge([src_stream,\n",
    "                         trg_stream],\n",
    "                         ('source', 'target'))\n",
    "\n",
    "# Filter sequences that are too long\n",
    "training_stream = Filter(training_stream,\n",
    "                         predicate=_too_long(seq_len=exp_config['seq_len']))\n",
    "\n",
    "# sampling_transformer = MTSampleStreamTransformer(sampling_func, fake_score, num_samples=5)\n",
    "sampling_transformer = MTSampleStreamTransformer(sampling_func, sentence_level_bleu, num_samples=exp_config['n_samples'])\n",
    "\n",
    "training_stream = Mapping(training_stream, sampling_transformer, add_sources=('samples', 'scores'))\n",
    "\n",
    "\n",
    "class FlattenSamples(Transformer):\n",
    "    \"\"\"Adds padding to variable-length sequences.\n",
    "\n",
    "    When your batches consist of variable-length sequences, use this class\n",
    "    to equalize lengths by adding zero-padding. To distinguish between\n",
    "    data and padding masks can be produced. For each data source that is\n",
    "    masked, a new source will be added. This source will have the name of\n",
    "    the original source with the suffix ``_mask`` (e.g. ``features_mask``).\n",
    "\n",
    "    Elements of incoming batches will be treated as numpy arrays (i.e.\n",
    "    using `numpy.asarray`). If they have more than one dimension,\n",
    "    all dimensions except length, that is the first one, must be equal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_stream : :class:`AbstractDataStream` instance\n",
    "        The data stream to wrap\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data_stream, **kwargs):\n",
    "        if data_stream.produces_examples:\n",
    "            raise ValueError('the wrapped data stream must produce batches of '\n",
    "                             'examples, not examples')\n",
    "        super(FlattenSamples, self).__init__(\n",
    "            data_stream, produces_examples=False, **kwargs)\n",
    "\n",
    "#         if mask_dtype is None:\n",
    "#             self.mask_dtype = config.floatX\n",
    "#         else:\n",
    "#             self.mask_dtype = mask_dtype\n",
    "\n",
    "    @property\n",
    "    def sources(self):\n",
    "        return self.data_stream.sources\n",
    "#         sources = []\n",
    "#         for source in self.data_stream.sources:\n",
    "#             sources.append(source)\n",
    "#             if source in self.mask_sources:\n",
    "#                 sources.append(source + '_mask')\n",
    "#         return tuple(sources)\n",
    "\n",
    "    def transform_batch(self, batch):\n",
    "        batch_with_flattened_samples = []\n",
    "        for i, (source, source_batch) in enumerate(\n",
    "                zip(self.data_stream.sources, batch)):\n",
    "#             if source not in self.mask_sources:\n",
    "#                 batch_with_masks.append(source_batch)\n",
    "#                 continue\n",
    "            if source == 'samples':\n",
    "                flattened_samples = []\n",
    "                for ins in source_batch:\n",
    "                    for sample in ins:\n",
    "                        flattened_samples.append(sample)\n",
    "                batch_with_flattened_samples.append(flattened_samples)\n",
    "            else:\n",
    "                batch_with_flattened_samples.append(source_batch)\n",
    "\n",
    "        return tuple(batch_with_flattened_samples)\n",
    "\n",
    "\n",
    "class CopySourceNTimes(Transformer):\n",
    "    \"\"\"Duplicate the source N times to match the number of samples\n",
    "\n",
    "    We need this transformer because the attention model expects one source sequence for each\n",
    "    target sequence, but in the sampling case there are effectively (instances*sample_size) target sequences\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_stream : :class:`AbstractDataStream` instance\n",
    "        The data stream to wrap\n",
    "    n_samples : int -- the number of samples that were generated for each source sequence\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data_stream, n_samples=5, **kwargs):\n",
    "        if data_stream.produces_examples:\n",
    "            raise ValueError('the wrapped data stream must produce batches of '\n",
    "                             'examples, not examples')\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "        super(CopySourceNTimes, self).__init__(\n",
    "            data_stream, produces_examples=False, **kwargs)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def sources(self):\n",
    "        return self.data_stream.sources\n",
    "\n",
    "    def transform_batch(self, batch):\n",
    "        batch_with_expanded_source = []\n",
    "        for i, (source, source_batch) in enumerate(\n",
    "                zip(self.data_stream.sources, batch)):\n",
    "            if source == 'source':\n",
    "#                 copy each source seqoyuence self.n_samples times, but keep the tensor 2d\n",
    "\n",
    "                expanded_source = []\n",
    "                for ins in source_batch:\n",
    "                    expanded_source.extend([ins for _ in range(self.n_samples)])\n",
    "\n",
    "                batch_with_expanded_source.append(expanded_source)\n",
    "            else:\n",
    "                batch_with_expanded_source.append(source_batch)\n",
    "\n",
    "        return tuple(batch_with_expanded_source)\n",
    "\n",
    "\n",
    "\n",
    "# Replace out of vocabulary tokens with unk token\n",
    "# training_stream = Mapping(training_stream,\n",
    "#                  _oov_to_unk(src_vocab_size=exp_config['src_vocab_size'],\n",
    "#                              trg_vocab_size=exp_config['trg_vocab_size'],\n",
    "#                              unk_id=exp_config['unk_id']))\n",
    "\n",
    "# Build a batched version of stream to read k batches ahead\n",
    "training_stream = Batch(training_stream,\n",
    "               iteration_scheme=ConstantScheme(\n",
    "                   exp_config['batch_size']*exp_config['sort_k_batches']))\n",
    "\n",
    "# Sort all samples in the read-ahead batch\n",
    "training_stream = Mapping(training_stream, SortMapping(_length))\n",
    "\n",
    "# Convert it into a stream again\n",
    "training_stream = Unpack(training_stream)\n",
    "\n",
    "# Construct batches from the stream with specified batch size\n",
    "training_stream = Batch(\n",
    "    training_stream, iteration_scheme=ConstantScheme(exp_config['batch_size']))\n",
    "\n",
    "# Pad sequences that are short\n",
    "# IDEA: add a transformer which flattens the target samples before we add the mask\n",
    "flat_sample_stream = FlattenSamples(training_stream)\n",
    "\n",
    "expanded_source_stream = CopySourceNTimes(flat_sample_stream, n_samples=exp_config['n_samples'])\n",
    "\n",
    "# TODO: some sources can be excluded from the padding Op, but since blocks matches sources with input variable\n",
    "# TODO: names, it's not critical\n",
    "masked_stream = PaddingWithEOS(\n",
    "    expanded_source_stream, [exp_config['src_vocab_size'] - 1, exp_config['trg_vocab_size'] - 1])\n",
    "\n",
    "\n",
    "def create_model(encoder, decoder):\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    source_sentence = tensor.lmatrix('source')\n",
    "    source_sentence_mask = tensor.matrix('source_mask')\n",
    "\n",
    "#     target_samples = tensor.tensor3('samples').astype('int64')\n",
    "#     target_samples_mask = tensor.tensor3('target_samples_mask').astype('int64')\n",
    "    samples = tensor.lmatrix('samples')\n",
    "    samples_mask = tensor.matrix('samples_mask')\n",
    "\n",
    "    # scores is (batch, samples)\n",
    "    scores = tensor.matrix('scores')\n",
    "    # We don't need a scores mask because there should be the same number of scores for each instance\n",
    "    # num samples is a hyperparameter of the model\n",
    "\n",
    "    # the name is important to make sure pre-trained params get loaded correctly\n",
    "#     decoder.name = 'decoder'\n",
    "\n",
    "    # This is the part that is different for the MinimumRiskSequenceGenerator\n",
    "    cost = decoder.expected_cost(\n",
    "        encoder.apply(source_sentence, source_sentence_mask),\n",
    "        source_sentence_mask, samples, samples_mask, scores)\n",
    "\n",
    "\n",
    "    return cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
