{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "from theano import tensor\n",
    "from toolz import merge\n",
    "import os\n",
    "\n",
    "import numpy\n",
    "\n",
    "from fuel.datasets import IterableDataset\n",
    "from fuel.transformers import Merge\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from blocks.bricks import (Tanh, Maxout, Linear, FeedforwardSequence,\n",
    "                           Bias, Initializable, MLP)\n",
    "from blocks.bricks.attention import SequenceContentAttention\n",
    "from blocks.bricks.base import application\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Bidirectional\n",
    "from blocks.bricks.sequence_generators import (\n",
    "    LookupFeedback, Readout, SoftmaxEmitter,\n",
    "    SequenceGenerator)\n",
    "from blocks.roles import add_role, WEIGHT\n",
    "from blocks.utils import shared_floatx_nans\n",
    "\n",
    "from machine_translation.models import MinRiskSequenceGenerator\n",
    "\n",
    "from picklable_itertools.extras import equizip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an NMT decoder which has access to image features via the target-side initial state\n",
    "\n",
    "# IDEA: subclass attention recurrent, and add one more context\n",
    "# -- could directly push the context onto attention_recurrent.context_names?\n",
    "\n",
    "# it's ok to add directly to the contexts of the recurrent transition, since that's what will be using them anyway,\n",
    "# TEST 1: what happens when we directly add the image features to the kwargs that we pass to sequence_generator.cost?\n",
    "# note this is similar to IMT, since we're trying to modify the decoder initial state\n",
    "\n",
    "# The kwargs do get passed through to the recurrent transition, so this should work\n",
    "\n",
    "# AttentionRecurrent gets created in the SequenceGenerator init(), which then calls BaseSequenceGenerator\n",
    "# Subclass SequenceGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add one more source for the images\n",
    "\n",
    "# get the MT datastream in the standard way, then add the new source using Merge\n",
    "# -- the problem with this is all the operations we do on the stream beforehand\n",
    "\n",
    "# as long as the arrays fit in memory, we should be able to use iterable dataset\n",
    "\n",
    "TRAIN_IMAGE_FEATURES = '/media/1tb_drive/multilingual-multimodal/flickr30k/img_features/f30k-translational-newsplits/train.npz'\n",
    "DEV_IMAGE_FEATURES = '/media/1tb_drive/multilingual-multimodal/flickr30k/img_features/f30k-translational-newsplits/dev.npz'\n",
    "TEST_IMAGE_FEATURES = '/media/1tb_drive/multilingual-multimodal/flickr30k/img_features/f30k-translational-newsplits/test.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the prototype config for NMT experiment with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BASEDIR = '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "          '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/'\n",
    "#best_bleu_model_1455464992_BLEU31.61.npz\n",
    "\n",
    "exp_config = {\n",
    "    'src_vocab_size': 20000,\n",
    "    'trg_vocab_size': 20000,\n",
    "    'enc_embed': 300,\n",
    "    'dec_embed': 300,\n",
    "    'enc_nhids': 800,\n",
    "    'dec_nhids': 800,\n",
    "    'src_vocab': os.path.join(BASEDIR, 'vocab.en-de.en.pkl'),\n",
    "    'trg_vocab': os.path.join(BASEDIR, 'vocab.en-de.de.pkl'),\n",
    "    'src_data': os.path.join(BASEDIR, 'training_data/train.en.tok.shuf'),\n",
    "    'trg_data': os.path.join(BASEDIR, 'training_data/train.de.tok.shuf'),\n",
    "    'unk_id':1,\n",
    "    # Bleu script that will be used (moses multi-perl in this case)\n",
    "    'bleu_script': '/home/chris/projects/neural_mt/test_data/sample_experiment/tiny_demo_dataset/multi-bleu.perl',\n",
    "\n",
    "    # Optimization related ----------------------------------------------------\n",
    "    # Batch size\n",
    "    'batch_size': 40,\n",
    "    # This many batches will be read ahead and sorted\n",
    "    'sort_k_batches': 10,\n",
    "    # Optimization step rule\n",
    "    'step_rule': 'AdaDelta',\n",
    "    # Gradient clipping threshold\n",
    "    'step_clipping': 1.,\n",
    "    # Std of weight initialization\n",
    "    'weight_scale': 0.01,\n",
    "    'seq_len': 40,\n",
    "    # Beam-size\n",
    "    'beam_size': 10,\n",
    "    'dropout': 0.3,\n",
    "    'weight_noise_ff': False,\n",
    "\n",
    "    # Maximum number of updates\n",
    "    'finish_after': 1000000,\n",
    "\n",
    "    # Reload model from files if exist\n",
    "    'reload': False,\n",
    "\n",
    "    # Save model after this many updates\n",
    "    'save_freq': 500,\n",
    "\n",
    "    # Show samples from model after this many updates\n",
    "    'sampling_freq': 1000,\n",
    "\n",
    "    # Show this many samples at each sampling\n",
    "    'hook_samples': 5,\n",
    "\n",
    "    # Validate bleu after this many updates\n",
    "    'bleu_val_freq': 50,\n",
    "    # Normalize cost according to sequence length after beam-search\n",
    "    'normalized_bleu': True,\n",
    "    \n",
    "    'saveto': '/media/1tb_drive/test_min_risk_model_save',\n",
    "    'model_save_directory': 'test_image_context_features_model_save',\n",
    "    \n",
    "    # Validation set source file\n",
    "    'val_set': '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/dev.en.tok',\n",
    "\n",
    "    # Validation set gold file\n",
    "    'val_set_grndtruth': '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/dev.de.tok',\n",
    "\n",
    "    # Print validation output to file\n",
    "    'output_val_set': True,\n",
    "\n",
    "    # Validation output file\n",
    "    'val_set_out': '/media/1tb_drive/test_min_risk_model_save/validation_out.txt',\n",
    "    'val_burn_in': 5000,\n",
    "\n",
    "    #     'saved_parameters': '/media/1tb_drive/multilingual-multimodal/flickr30k/train/processed/BERTHA-TEST_wmt-multimodal_internal_data_dropout0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/best_bleu_model_1455410311_BLEU30.38.npz',\n",
    "\n",
    "    # NEW PARAMS FOR ADDING CONTEXT FEATURES\n",
    "    'context_features': '/media/1tb_drive/multilingual-multimodal/flickr30k/img_features/f30k-translational-newsplits/train.npz', \n",
    "    'val_context_features': '/media/1tb_drive/multilingual-multimodal/flickr30k/img_features/f30k-translational-newsplits/dev.npz',\n",
    "    # the dimensionality of the context features\n",
    "    'context_dim': 4096\n",
    "    \n",
    "    # NEW PARAM FOR MIN RISK\n",
    "#     'n_samples': 100\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from machine_translation.stream import _ensure_special_tokens, _length, PaddingWithEOS, _oov_to_unk, _too_long\n",
    "\n",
    "def get_tr_stream_with_context_features(src_vocab, trg_vocab, src_data, trg_data, context_features,\n",
    "                  src_vocab_size=30000, trg_vocab_size=30000, unk_id=1,\n",
    "                  seq_len=50, batch_size=80, sort_k_batches=12, **kwargs):\n",
    "    \"\"\"Prepares the training data stream.\"\"\"\n",
    "\n",
    "    def _get_np_array(filename):\n",
    "        return numpy.load(filename)['arr_0']\n",
    "    \n",
    "    # Load dictionaries and ensure special tokens exist\n",
    "    src_vocab = _ensure_special_tokens(\n",
    "        src_vocab if isinstance(src_vocab, dict)\n",
    "        else cPickle.load(open(src_vocab)),\n",
    "        bos_idx=0, eos_idx=src_vocab_size - 1, unk_idx=unk_id)\n",
    "    trg_vocab = _ensure_special_tokens(\n",
    "        trg_vocab if isinstance(trg_vocab, dict) else\n",
    "        cPickle.load(open(trg_vocab)),\n",
    "        bos_idx=0, eos_idx=trg_vocab_size - 1, unk_idx=unk_id)\n",
    "\n",
    "    # Get text files from both source and target\n",
    "    src_dataset = TextFile([src_data], src_vocab, None)\n",
    "    trg_dataset = TextFile([trg_data], trg_vocab, None)\n",
    "\n",
    "    # Merge them to get a source, target pair\n",
    "    stream = Merge([src_dataset.get_example_stream(),\n",
    "                    trg_dataset.get_example_stream()],\n",
    "                   ('source', 'target'))\n",
    "\n",
    "    # Filter sequences that are too long\n",
    "    stream = Filter(stream,\n",
    "                    predicate=_too_long(seq_len=seq_len))\n",
    "    \n",
    "  \n",
    "    # Replace out of vocabulary tokens with unk token\n",
    "    # TODO: doesn't the TextFile stream do this anyway?\n",
    "    stream = Mapping(stream,\n",
    "                     _oov_to_unk(src_vocab_size=src_vocab_size,\n",
    "                                 trg_vocab_size=trg_vocab_size,\n",
    "                                 unk_id=unk_id))\n",
    "\n",
    "    # now add the source with the image features\n",
    "    # create the image datastream (iterate over a file line-by-line)\n",
    "    train_features = _get_np_array(context_features)\n",
    "    train_feature_dataset = IterableDataset(train_features)\n",
    "    train_image_stream = DataStream(train_feature_dataset)\n",
    "\n",
    "    stream = Merge([stream, train_image_stream], ('source', 'target', 'initial_context'))\n",
    "    \n",
    "    # Build a batched version of stream to read k batches ahead\n",
    "    stream = Batch(stream,\n",
    "                   iteration_scheme=ConstantScheme(\n",
    "                       batch_size*sort_k_batches))\n",
    "\n",
    "    # Sort all samples in the read-ahead batch\n",
    "    stream = Mapping(stream, SortMapping(_length))\n",
    "\n",
    "    # Convert it into a stream again\n",
    "    stream = Unpack(stream)\n",
    "\n",
    "    # Construct batches from the stream with specified batch size\n",
    "    stream = Batch(\n",
    "        stream, iteration_scheme=ConstantScheme(batch_size))\n",
    "\n",
    "    # Pad sequences that are short\n",
    "    masked_stream = PaddingWithEOS(\n",
    "        stream, [src_vocab_size - 1, trg_vocab_size - 1], mask_sources=('source', 'target'))\n",
    "\n",
    "    return masked_stream, src_vocab, trg_vocab\n",
    "\n",
    "\n",
    "# Remember that the BleuValidator does hackish stuff to get target set information from the main_loop data_stream\n",
    "# using all kwargs here makes it more clear that this function is always called with get_dev_stream(**config_dict)\n",
    "def get_dev_stream_with_context_features(val_context_features=None, val_set=None, src_vocab=None,\n",
    "                                         src_vocab_size=30000, unk_id=1, **kwargs):\n",
    "    \"\"\"Setup development set stream if necessary.\"\"\"\n",
    "    \n",
    "    def _get_np_array(filename):\n",
    "        return numpy.load(filename)['arr_0']\n",
    "    \n",
    "    \n",
    "    dev_stream = None\n",
    "    if val_set is not None and src_vocab is not None:\n",
    "        src_vocab = _ensure_special_tokens(\n",
    "            src_vocab if isinstance(src_vocab, dict) else\n",
    "            cPickle.load(open(src_vocab)),\n",
    "            bos_idx=0, eos_idx=src_vocab_size - 1, unk_idx=unk_id)\n",
    "        \n",
    "        # TODO: how is the dev dataset used without the context features?\n",
    "        dev_dataset = TextFile([val_set], src_vocab, None)\n",
    "        \n",
    "        # now add the source with the image features\n",
    "        # create the image datastream (iterate over a file line-by-line)\n",
    "        con_features = _get_np_array(val_context_features)\n",
    "        con_feature_dataset = IterableDataset(con_features)\n",
    "        valid_image_stream = DataStream(con_feature_dataset)\n",
    "        \n",
    "        dev_stream = DataStream(dev_dataset)\n",
    "        dev_stream = Merge([dev_dataset.get_example_stream(),\n",
    "                            valid_image_stream], ('source', 'initial_context'))\n",
    "#         dev_stream = dev_stream.get_example_stream()\n",
    "\n",
    "    return dev_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Running Neural Machine Translation in mode: train\n"
     ]
    }
   ],
   "source": [
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping)\n",
    "\n",
    "from six.moves import cPickle\n",
    "\n",
    "# setting up the experiment\n",
    "    \n",
    "# args = parser.parse_args()\n",
    "# arg_dict = vars(args)\n",
    "# configuration_file = arg_dict['exp_config']\n",
    "# mode = arg_dict['mode']\n",
    "\n",
    "mode = 'train'\n",
    "logger.info('Running Neural Machine Translation in mode: {}'.format(mode))\n",
    "# config_obj = configurations.get_config(configuration_file)\n",
    "config_obj = exp_config\n",
    "\n",
    "# add the config file name into config_obj\n",
    "# config_obj['config_file'] = configuration_file\n",
    "# logger.info(\"Model Configuration:\\n{}\".format(pprint.pformat(config_obj)))\n",
    "\n",
    "train_stream, source_vocab, target_vocab = get_tr_stream_with_context_features(**config_obj)\n",
    "dev_stream = get_dev_stream_with_context_features(**config_obj)\n",
    "\n",
    "# if mode == 'train':\n",
    "    # Get data streams and call main\n",
    "#     main(config_obj, get_tr_stream(**config_obj),\n",
    "#          get_dev_stream(**config_obj), bokeh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fuel.transformers.Merge"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dev_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = next(train_stream.get_epoch_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(40, 13), (40, 13), (40, 9), (40, 9), (40, 4096)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('source', 'source_mask', 'target', 'target_mask', 'initial_context')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stream.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('source', 'initial_context')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_stream.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRUInitialStateWithInitialStateContext(GatedRecurrent):\n",
    "    \"\"\"Gated Recurrent with special initial state.\n",
    "\n",
    "    Initial state of Gated Recurrent is set by an MLP that conditions on the\n",
    "    last hidden state of the bidirectional encoder, applies an affine\n",
    "    transformation followed by a tanh non-linearity to set initial state.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, attended_dim, context_dim, **kwargs):\n",
    "        super(GRUInitialStateWithInitialStateContext, self).__init__(**kwargs)\n",
    "        self.attended_dim = attended_dim\n",
    "        self.context_dim = context_dim\n",
    "\n",
    "        self.initial_transformer = MLP(activations=[Tanh(),Tanh(),Tanh()],\n",
    "                                       dims=[attended_dim + context_dim, 1000, 500, self.dim],\n",
    "                                       name='state_initializer')\n",
    "        self.children.append(self.initial_transformer)\n",
    "  \n",
    "    # WORKING: add the images as another context to the recurrent transition\n",
    "    # THINKING: how to best combine the image info with the source info?\n",
    "    @application\n",
    "    def initial_states(self, batch_size, *args, **kwargs):\n",
    "        attended = kwargs['attended']\n",
    "        context = kwargs['initial_state_context']\n",
    "        attended_reverse_final_state = attended[0, :, -self.attended_dim:]\n",
    "        concat_attended_and_context = tensor.concatenate([attended_reverse_final_state, context], axis=1)\n",
    "        initial_state = self.initial_transformer.apply(concat_attended_and_context)\n",
    "        return initial_state\n",
    "\n",
    "    def _allocate(self):\n",
    "        self.parameters.append(shared_floatx_nans((self.dim, self.dim),\n",
    "                               name='state_to_state'))\n",
    "        self.parameters.append(shared_floatx_nans((self.dim, 2 * self.dim),\n",
    "                               name='state_to_gates'))\n",
    "        for i in range(2):\n",
    "            if self.parameters[i]:\n",
    "                add_role(self.parameters[i], WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "from theano import tensor\n",
    "from six import add_metaclass\n",
    "\n",
    "from blocks.bricks import (Brick, Initializable, Sequence,\n",
    "                           Feedforward, Linear, Tanh)\n",
    "from blocks.bricks.base import lazy, application\n",
    "from blocks.bricks.parallel import Parallel, Distribute\n",
    "from blocks.bricks.recurrent import recurrent, BaseRecurrent\n",
    "from blocks.utils import dict_union, dict_subset, pack\n",
    "\n",
    "from blocks.bricks.attention import AttentionRecurrent\n",
    "\n",
    "class InitialContextAttentionRecurrent(AttentionRecurrent):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(InitialContextAttentionRecurrent, self).__init__(*args, **kwargs)\n",
    "#         print('CONTEXT NAMES:')\n",
    "#         print(self._context_names)\n",
    "#         self._context_names.append('initial_state_context')\n",
    "#         print('CONTEXT NAMES:')\n",
    "#         print(self._context_names)\n",
    "        \n",
    "#     @application\n",
    "#     def compute_states(self, **kwargs):\n",
    "#         r\"\"\"Compute current states when glimpses have already been computed.\n",
    "\n",
    "#         Combines an application of the `distribute` that alter the\n",
    "#         sequential inputs of the wrapped transition and an application of\n",
    "#         the wrapped transition. All unknown keyword arguments go to\n",
    "#         the wrapped transition.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         \\*\\*kwargs\n",
    "#             Should contain everything what `self.transition` needs\n",
    "#             and in addition the current glimpses.\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         current_states : list of :class:`~tensor.TensorVariable`\n",
    "#             Current states computed by `self.transition`.\n",
    "\n",
    "#         \"\"\"\n",
    "#         # make sure we are not popping the mask\n",
    "#         normal_inputs = [name for name in self._sequence_names\n",
    "#                          if 'mask' not in name]\n",
    "#         sequences = dict_subset(kwargs, normal_inputs, pop=True)\n",
    "#         glimpses = dict_subset(kwargs, self._glimpse_names, pop=True)\n",
    "#         if self.add_contexts:\n",
    "#             kwargs.pop(self.attended_name)\n",
    "#             # attended_mask_name can be optional\n",
    "#             kwargs.pop(self.attended_mask_name, None)\n",
    "            \n",
    "        \n",
    "\n",
    "#         sequences.update(self.distribute.apply(\n",
    "#             as_dict=True, **dict_subset(dict_union(sequences, glimpses),\n",
    "#                                         self.distribute.apply.inputs)))\n",
    "#         current_states = self.transition.apply(\n",
    "#             iterate=False, as_list=True,\n",
    "#             **dict_union(sequences, kwargs))\n",
    "#         return current_states\n",
    "    \n",
    "#     @recurrent\n",
    "#     def do_apply(self, **kwargs):\n",
    "#         r\"\"\"Process a sequence attending the attended context every step.\n",
    "\n",
    "#         In addition to the original sequence this method also requires\n",
    "#         its preprocessed version, the one computed by the `preprocess`\n",
    "#         method of the attention mechanism. Unknown keyword arguments\n",
    "#         are passed to the wrapped transition.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         \\*\\*kwargs\n",
    "#             Should contain current inputs, previous step states, contexts,\n",
    "#             the preprocessed attended context, previous step glimpses.\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         outputs : list of :class:`~tensor.TensorVariable`\n",
    "#             The current step states and glimpses.\n",
    "\n",
    "#         \"\"\"\n",
    "#         attended = kwargs[self.attended_name]\n",
    "#         preprocessed_attended = kwargs.pop(self.preprocessed_attended_name)\n",
    "#         attended_mask = kwargs.get(self.attended_mask_name)\n",
    "#         sequences = dict_subset(kwargs, self._sequence_names, pop=True,\n",
    "#                                 must_have=False)\n",
    "#         states = dict_subset(kwargs, self._state_names, pop=True)\n",
    "#         glimpses = dict_subset(kwargs, self._glimpse_names, pop=True)\n",
    "\n",
    "#         current_glimpses = self.take_glimpses(\n",
    "#             as_dict=True,\n",
    "#             **dict_union(\n",
    "#                 states, glimpses,\n",
    "#                 {self.attended_name: attended,\n",
    "#                  self.attended_mask_name: attended_mask,\n",
    "#                  self.preprocessed_attended_name: preprocessed_attended}))\n",
    "#         current_states = self.compute_states(\n",
    "#             as_list=True,\n",
    "#             **dict_union(sequences, states, current_glimpses, kwargs))\n",
    "#         return current_states + list(current_glimpses.values())\n",
    "    \n",
    "    \n",
    "#     @do_apply.property('contexts')\n",
    "#     def do_apply_contexts(self):\n",
    "#         return self._context_names + [self.preprocessed_attended_name] + ['initial_state_context']\n",
    "#         return self._context_names + [self.preprocessed_attended_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WORKING: sequence generator which uses the contexts properly\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "from six import add_metaclass\n",
    "from theano import tensor\n",
    "\n",
    "from blocks.bricks import Initializable, Random, Bias, NDimensionalSoftmax\n",
    "from blocks.bricks.base import application, Brick, lazy\n",
    "from blocks.bricks.parallel import Fork, Merge\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.bricks.recurrent import recurrent\n",
    "from blocks.bricks.attention import (\n",
    "    AbstractAttentionRecurrent, AttentionRecurrent)\n",
    "from blocks.roles import add_role, COST\n",
    "from blocks.utils import dict_union, dict_subset\n",
    "\n",
    "from blocks.bricks.sequence_generators import BaseSequenceGenerator\n",
    "\n",
    "class InitialContextSequenceGenerator(BaseSequenceGenerator):\n",
    "    \n",
    "    def __init__(self, readout, transition, attention=None,\n",
    "                 add_contexts=True, **kwargs):\n",
    "        normal_inputs = [name for name in transition.apply.sequences\n",
    "                         if 'mask' not in name]\n",
    "        kwargs.setdefault('fork', Fork(normal_inputs))\n",
    "        if attention:\n",
    "            transition = InitialContextAttentionRecurrent(\n",
    "#             transition = AttentionRecurrent(\n",
    "                transition, attention,\n",
    "                add_contexts=add_contexts, name=\"att_trans\")\n",
    "        else:\n",
    "            transition = FakeAttentionRecurrent(transition,\n",
    "                                                name=\"with_fake_attention\")\n",
    "        super(InitialContextSequenceGenerator, self).__init__(\n",
    "            readout, transition, **kwargs)\n",
    "    \n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         self.softmax = NDimensionalSoftmax()\n",
    "#         super(InitialContextSequenceGenerator, self).__init__(*args, **kwargs)\n",
    "#         self.children.append(self.softmax)\n",
    "\n",
    "    @application\n",
    "    def cost_matrix(self, application_call, outputs, mask=None, **kwargs):\n",
    "        \"\"\"Returns generation costs for output sequences.\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        :meth:`cost` : Scalar cost.\n",
    "\n",
    "        \"\"\"\n",
    "        # We assume the data has axes (time, batch, features, ...)\n",
    "        batch_size = outputs.shape[1]\n",
    "\n",
    "        # Prepare input for the iterative part\n",
    "        states = dict_subset(kwargs, self._state_names, must_have=False)\n",
    "        # masks in context are optional (e.g. `attended_mask`)\n",
    "#         contexts = dict_subset(kwargs, self._context_names, must_have=False)\n",
    "        contexts = dict_subset(kwargs, self._context_names, must_have=False)\n",
    "        contexts['initial_state_context'] = kwargs['initial_state_context']\n",
    "    \n",
    "        feedback = self.readout.feedback(outputs)\n",
    "        inputs = self.fork.apply(feedback, as_dict=True)\n",
    "\n",
    "        # Run the recurrent network\n",
    "        results = self.transition.apply(\n",
    "            mask=mask, return_initial_states=True, as_dict=True,\n",
    "            **dict_union(inputs, states, contexts))\n",
    "\n",
    "        # Separate the deliverables. The last states are discarded: they\n",
    "        # are not used to predict any output symbol. The initial glimpses\n",
    "        # are discarded because they are not used for prediction.\n",
    "        # Remember, glimpses are computed _before_ output stage, states are\n",
    "        # computed after.\n",
    "        states = {name: results[name][:-1] for name in self._state_names}\n",
    "        glimpses = {name: results[name][1:] for name in self._glimpse_names}\n",
    "\n",
    "        # Compute the cost\n",
    "        feedback = tensor.roll(feedback, 1, 0)\n",
    "        feedback = tensor.set_subtensor(\n",
    "            feedback[0],\n",
    "            self.readout.feedback(self.readout.initial_outputs(batch_size)))\n",
    "        readouts = self.readout.readout(\n",
    "            feedback=feedback, **dict_union(states, glimpses, contexts))\n",
    "        costs = self.readout.cost(readouts, outputs)\n",
    "        if mask is not None:\n",
    "            costs *= mask\n",
    "\n",
    "        for name, variable in list(glimpses.items()) + list(states.items()):\n",
    "            application_call.add_auxiliary_variable(\n",
    "                variable.copy(), name=name)\n",
    "\n",
    "        # This variables can be used to initialize the initial states of the\n",
    "        # next batch using the last states of the current batch.\n",
    "        for name in self._state_names + self._glimpse_names:\n",
    "            application_call.add_auxiliary_variable(\n",
    "                results[name][-1].copy(), name=name+\"_final_value\")\n",
    "\n",
    "        return costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano import tensor\n",
    "from toolz import merge\n",
    "\n",
    "from blocks.bricks import (Tanh, Maxout, Linear, FeedforwardSequence,\n",
    "                           Bias, Initializable, MLP)\n",
    "from blocks.bricks.attention import SequenceContentAttention\n",
    "from blocks.bricks.base import application\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Bidirectional\n",
    "from blocks.bricks.sequence_generators import (\n",
    "    LookupFeedback, Readout, SoftmaxEmitter,\n",
    "    SequenceGenerator)\n",
    "from blocks.roles import add_role, WEIGHT\n",
    "from blocks.utils import shared_floatx_nans\n",
    "\n",
    "from machine_translation.models import MinRiskSequenceGenerator\n",
    "\n",
    "from picklable_itertools.extras import equizip\n",
    "\n",
    "from machine_translation.model import LookupFeedbackWMT15, InitializableFeedforwardSequence\n",
    "\n",
    "# TODO: a lot of code was duplicated here during speedy prototyping -- CLEAN UP\n",
    "class InitialContextDecoder(Initializable):\n",
    "    \"\"\"\n",
    "    Decoder which incorporates context features into the target-side initial state\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    vocab_size: int\n",
    "    embedding_dim: int\n",
    "    representation_dim: int\n",
    "    theano_seed: int\n",
    "    loss_function: str : {'cross_entropy'(default) | 'min_risk'}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, state_dim,\n",
    "                 representation_dim, context_dim, theano_seed=None, loss_function='cross_entropy', **kwargs):\n",
    "        super(InitialContextDecoder, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.representation_dim = representation_dim\n",
    "        self.theano_seed = theano_seed\n",
    "\n",
    "        # Initialize gru with special initial state\n",
    "        self.transition = GRUInitialStateWithInitialStateContext(\n",
    "            attended_dim=state_dim, context_dim=context_dim, dim=state_dim,\n",
    "            activation=Tanh(), name='decoder')\n",
    "\n",
    "        # Initialize the attention mechanism\n",
    "        self.attention = SequenceContentAttention(\n",
    "            state_names=self.transition.apply.states,\n",
    "            attended_dim=representation_dim,\n",
    "            match_dim=state_dim, name=\"attention\")\n",
    "\n",
    "        # Initialize the readout, note that SoftmaxEmitter emits -1 for\n",
    "        # initial outputs which is used by LookupFeedBackWMT15\n",
    "        readout = Readout(\n",
    "            source_names=['states', 'feedback',\n",
    "                          # Chris: it's key that we're taking the first output of self.attention.take_glimpses.outputs\n",
    "                          # Chris: the first output is the weighted avgs, the second is the weights in (batch, time)\n",
    "                          self.attention.take_glimpses.outputs[0]],\n",
    "            readout_dim=self.vocab_size,\n",
    "            emitter=SoftmaxEmitter(initial_output=-1, theano_seed=theano_seed),\n",
    "            feedback_brick=LookupFeedbackWMT15(vocab_size, embedding_dim),\n",
    "            post_merge=InitializableFeedforwardSequence(\n",
    "                [Bias(dim=state_dim, name='maxout_bias').apply,\n",
    "                 Maxout(num_pieces=2, name='maxout').apply,\n",
    "                 Linear(input_dim=state_dim / 2, output_dim=embedding_dim,\n",
    "                        use_bias=False, name='softmax0').apply,\n",
    "                 Linear(input_dim=embedding_dim, name='softmax1').apply]),\n",
    "            merged_dim=state_dim)\n",
    "\n",
    "        # Build sequence generator accordingly\n",
    "        if loss_function == 'cross_entropy':\n",
    "            self.sequence_generator = InitialContextSequenceGenerator(\n",
    "                readout=readout,\n",
    "                transition=self.transition,\n",
    "                attention=self.attention,\n",
    "                fork=Fork([name for name in self.transition.apply.sequences\n",
    "                           if name != 'mask'], prototype=Linear())\n",
    "            )\n",
    "#         elif loss_function == 'min_risk':\n",
    "#             self.sequence_generator = MinRiskSequenceGenerator(\n",
    "#                 readout=readout,\n",
    "#                 transition=self.transition,\n",
    "#                 attention=self.attention,\n",
    "#                 fork=Fork([name for name in self.transition.apply.sequences\n",
    "#                            if name != 'mask'], prototype=Linear())\n",
    "#             )\n",
    "            # the name is important, because it lets us match the brick hierarchy names for the vanilla SequenceGenerator\n",
    "            # to load pretrained models\n",
    "            self.sequence_generator.name = 'sequencegenerator'\n",
    "        else:\n",
    "            raise ValueError('The decoder does not support the loss function: {}'.format(loss_function))\n",
    "\n",
    "        self.children = [self.sequence_generator]\n",
    "\n",
    "    @application(inputs=['representation', 'source_sentence_mask',\n",
    "                         'target_sentence_mask', 'target_sentence', 'initial_state_context'],\n",
    "                 outputs=['cost'])\n",
    "    def cost(self, representation, source_sentence_mask,\n",
    "             target_sentence, target_sentence_mask, initial_state_context):\n",
    "\n",
    "        source_sentence_mask = source_sentence_mask.T\n",
    "        target_sentence = target_sentence.T\n",
    "        target_sentence_mask = target_sentence_mask.T\n",
    "\n",
    "        # Get the cost matrix\n",
    "        cost = self.sequence_generator.cost_matrix(**{\n",
    "            'mask': target_sentence_mask,\n",
    "            'outputs': target_sentence,\n",
    "            'attended': representation,\n",
    "            'attended_mask': source_sentence_mask,\n",
    "            'initial_state_context': initial_state_context\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return (cost * target_sentence_mask).sum() / \\\n",
    "            target_sentence_mask.shape[1]\n",
    "\n",
    "    # Note: this requires the decoder to be using sequence_generator which implements expected cost\n",
    "#     @application(inputs=['representation', 'source_sentence_mask',\n",
    "#                          'target_samples_mask', 'target_samples', 'scores'],\n",
    "#                  outputs=['cost'])\n",
    "#     def expected_cost(self, representation, source_sentence_mask, target_samples, target_samples_mask, scores,\n",
    "#                       **kwargs):\n",
    "#         return self.sequence_generator.expected_cost(representation,\n",
    "#                                                      source_sentence_mask,\n",
    "#                                                      target_samples, target_samples_mask, scores, **kwargs)\n",
    "\n",
    "\n",
    "    @application\n",
    "    def generate(self, source_sentence, representation, initial_state_context, **kwargs):\n",
    "        return self.sequence_generator.generate(\n",
    "            n_steps=2 * source_sentence.shape[1],\n",
    "            batch_size=source_sentence.shape[0],\n",
    "            attended=representation,\n",
    "            attended_mask=tensor.ones(source_sentence.shape).T,\n",
    "            initial_state_context=initial_state_context,\n",
    "            **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WORKING: make beam search and sampling work nicely with the new context\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "import signal\n",
    "import time\n",
    "import theano\n",
    "\n",
    "from blocks.extensions import SimpleExtension\n",
    "from blocks.search import BeamSearch\n",
    "from machine_translation.checkpoint import SaveLoadUtils\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# this is to let us use all of the sources in the fuel dev stream\n",
    "# without needing to explicitly filter them\n",
    "theano.config.on_unused_input = 'warn'\n",
    "\n",
    "\n",
    "class SamplingBase(object):\n",
    "    \"\"\"Utility class for BleuValidator and Sampler.\"\"\"\n",
    "\n",
    "    def _get_attr_rec(self, obj, attr):\n",
    "        return self._get_attr_rec(getattr(obj, attr), attr) \\\n",
    "            if hasattr(obj, attr) else obj\n",
    "\n",
    "    def _get_true_length(self, seq, vocab):\n",
    "        try:\n",
    "            return seq.tolist().index(vocab['</S>']) + 1\n",
    "        except ValueError:\n",
    "            return len(seq)\n",
    "\n",
    "    def _oov_to_unk(self, seq, vocab_size, unk_idx):\n",
    "        return [x if x < vocab_size else unk_idx for x in seq]\n",
    "\n",
    "    def _idx_to_word(self, seq, ivocab):\n",
    "        return \" \".join([ivocab.get(idx, \"<UNK>\") for idx in seq])\n",
    "\n",
    "    def _initialize_dataset_info(self):\n",
    "        # Get dictionaries, this may not be the practical way\n",
    "        sources = self._get_attr_rec(self.main_loop, 'data_stream')\n",
    "\n",
    "        # Load vocabularies and invert if necessary\n",
    "        # WARNING: Source and target indices from data stream\n",
    "        #  can be different\n",
    "#         if not hasattr(self, 'source_dataset'):\n",
    "#             self.source_dataset = sources.data_streams[0].dataset\n",
    "#         if not hasattr(self, 'target_dataset'):\n",
    "#             self.target_dataset = sources.data_streams[1].dataset\n",
    "        if not hasattr(self, 'src_vocab'):\n",
    "            self.src_vocab = self.source_dataset.dictionary\n",
    "        if not hasattr(self, 'trg_vocab'):\n",
    "            self.trg_vocab = self.target_dataset.dictionary\n",
    "        if not hasattr(self, 'src_ivocab'):\n",
    "            self.src_ivocab = {v: k for k, v in self.src_vocab.items()}\n",
    "        if not hasattr(self, 'trg_ivocab'):\n",
    "            self.trg_ivocab = {v: k for k, v in self.trg_vocab.items()}\n",
    "        if not hasattr(self, 'src_vocab_size'):\n",
    "            self.src_vocab_size = len(self.src_vocab)\n",
    "\n",
    "\n",
    "class Sampler(SimpleExtension, SamplingBase):\n",
    "    \"\"\"Random Sampling from model.\"\"\"\n",
    "\n",
    "    def __init__(self, model, data_stream, hook_samples=1,\n",
    "                 src_vocab=None, trg_vocab=None, src_ivocab=None,\n",
    "                 trg_ivocab=None, src_vocab_size=None, **kwargs):\n",
    "        super(Sampler, self).__init__(**kwargs)\n",
    "        self.model = model\n",
    "        self.hook_samples = hook_samples\n",
    "        self.data_stream = data_stream\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.src_ivocab = src_ivocab\n",
    "        self.trg_ivocab = trg_ivocab\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.is_synced = False\n",
    "\n",
    "        self.sampling_fn = model.get_theano_function()\n",
    "\n",
    "    def do(self, which_callback, *args):\n",
    "                # Get dictionaries, this may not be the practical way\n",
    "        sources = self._get_attr_rec(self.main_loop, 'data_stream')\n",
    "\n",
    "        # Load vocabularies and invert if necessary\n",
    "        # WARNING: Source and target indices from data stream\n",
    "        #  can be different\n",
    "        if not self.src_vocab:\n",
    "            self.src_vocab = sources.data_streams[0].dataset.dictionary\n",
    "        if not self.trg_vocab:\n",
    "            self.trg_vocab = sources.data_streams[1].dataset.dictionary\n",
    "        if not self.src_ivocab:\n",
    "            self.src_ivocab = {v: k for k, v in self.src_vocab.items()}\n",
    "        if not self.trg_ivocab:\n",
    "            self.trg_ivocab = {v: k for k, v in self.trg_vocab.items()}\n",
    "        if not self.src_vocab_size:\n",
    "            self.src_vocab_size = len(self.src_vocab)\n",
    "\n",
    "        # Randomly select source samples from the current batch\n",
    "        # WARNING: Source and target indices from data stream\n",
    "        #  can be different\n",
    "        batch = args[0]\n",
    "        batch_size = batch['source'].shape[0]\n",
    "        hook_samples = min(batch_size, self.hook_samples)\n",
    "\n",
    "        # TODO: this is problematic for boundary conditions, eg. last batch\n",
    "        sample_idx = numpy.random.choice(\n",
    "            batch_size, hook_samples, replace=False)\n",
    "        src_batch = batch[self.main_loop.data_stream.mask_sources[0]]\n",
    "        trg_batch = batch[self.main_loop.data_stream.mask_sources[1]]\n",
    "        context_batch = batch[self.main_loop.data_stream.sources[-1]]\n",
    "\n",
    "        input_ = src_batch[sample_idx, :]\n",
    "        target_ = trg_batch[sample_idx, :]\n",
    "        context_ = context_batch[sample_idx, :]\n",
    "\n",
    "\n",
    "        # Sample\n",
    "        print()\n",
    "        for i in range(hook_samples):\n",
    "            input_length = self._get_true_length(input_[i], self.src_vocab)\n",
    "            target_length = self._get_true_length(target_[i], self.trg_vocab)\n",
    "\n",
    "            inp = input_[i, :input_length]\n",
    "            context = context_[i]\n",
    "\n",
    "            # outputs of self.sampling_fn:\n",
    "            _1, outputs, _2, _3, costs = (self.sampling_fn(inp[None, :]), context[None, :])\n",
    "            outputs = outputs.flatten()\n",
    "            costs = costs.T\n",
    "\n",
    "            sample_length = self._get_true_length(outputs, self.trg_vocab)\n",
    "\n",
    "            print(\"Input : \", self._idx_to_word(input_[i][:input_length],\n",
    "                                                self.src_ivocab))\n",
    "            print(\"Target: \", self._idx_to_word(target_[i][:target_length],\n",
    "                                                self.trg_ivocab))\n",
    "            print(\"Sample: \", self._idx_to_word(outputs[:sample_length],\n",
    "                                                self.trg_ivocab))\n",
    "            print(\"Sample cost: \", costs[:sample_length].sum())\n",
    "            print()\n",
    "\n",
    "\n",
    "class BleuValidator(SimpleExtension, SamplingBase):\n",
    "    # TODO: a lot has been changed in NMT, sync respectively\n",
    "    # TODO: there is a mistake here when the source and target vocabulary sizes are different -- fix the \"\"Helpers\" section below\n",
    "    \"\"\"Implements early stopping based on BLEU score.\"\"\"\n",
    "\n",
    "    def __init__(self, source_sentence, initial_state_context, samples, model, data_stream,\n",
    "                 config, src_vocab=None, trg_vocab=None, n_best=1, track_n_models=1,\n",
    "                 normalize=True, **kwargs):\n",
    "        # TODO: change config structure\n",
    "        super(BleuValidator, self).__init__(**kwargs)\n",
    "        self.source_sentence = source_sentence\n",
    "        self.initial_context = initial_state_context\n",
    "        \n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        \n",
    "        self.samples = samples\n",
    "        self.model = model\n",
    "        self.data_stream = data_stream\n",
    "        self.config = config\n",
    "        self.n_best = n_best\n",
    "        self.track_n_models = track_n_models\n",
    "        self.normalize = normalize\n",
    "        self.verbose = config.get('val_set_out', None)\n",
    "\n",
    "        # Helpers\n",
    "        self.best_models = []\n",
    "        self.val_bleu_curve = []\n",
    "        self.beam_search = BeamSearch(samples=samples)\n",
    "        self.multibleu_cmd = ['perl', self.config['bleu_script'],\n",
    "                              self.config['val_set_grndtruth'], '<']\n",
    "\n",
    "        # Create saving directory if it does not exist\n",
    "        if not os.path.exists(self.config['saveto']):\n",
    "            os.makedirs(self.config['saveto'])\n",
    "\n",
    "        if self.config['reload']:\n",
    "            try:\n",
    "                bleu_score = numpy.load(os.path.join(self.config['saveto'],\n",
    "                                        'val_bleu_scores.npz'))\n",
    "                self.val_bleu_curve = bleu_score['bleu_scores'].tolist()\n",
    "\n",
    "                # Track n best previous bleu scores\n",
    "                for i, bleu in enumerate(\n",
    "                        sorted(self.val_bleu_curve, reverse=True)):\n",
    "                    if i < self.track_n_models:\n",
    "                        self.best_models.append(ModelInfo(bleu))\n",
    "                logger.info(\"BleuScores Reloaded\")\n",
    "            except:\n",
    "                logger.info(\"BleuScores not Found\")\n",
    "\n",
    "    def do(self, which_callback, *args):\n",
    "\n",
    "        # Track validation burn in\n",
    "        if self.main_loop.status['iterations_done'] <= \\\n",
    "                self.config['val_burn_in']:\n",
    "            return\n",
    "\n",
    "        # Evaluate the model\n",
    "        bleu_score = self._evaluate_model()\n",
    "        # add an entry to the log\n",
    "        self.main_loop.log.current_row['validation_set_bleu_score'] = bleu_score\n",
    "        # save if necessary\n",
    "        self._save_model(bleu_score)\n",
    "\n",
    "    def _evaluate_model(self):\n",
    "        # Set in the superclass -- SamplingBase\n",
    "        if not hasattr(self, 'target_dataset'):\n",
    "            self._initialize_dataset_info()\n",
    "        \n",
    "#         self.unk_sym = self.target_dataset.unk_token\n",
    "#         self.eos_sym = self.target_dataset.eos_token\n",
    "        \n",
    "        self.unk_sym = '<UNK>'\n",
    "        self.eos_sym = '</S>'\n",
    "        self.unk_idx = self.trg_vocab[self.unk_sym]\n",
    "        self.eos_idx = self.trg_vocab[self.eos_sym]\n",
    "\n",
    "        logger.info(\"Started Validation: \")\n",
    "        val_start_time = time.time()\n",
    "        mb_subprocess = Popen(self.multibleu_cmd, stdin=PIPE, stdout=PIPE)\n",
    "        total_cost = 0.0\n",
    "\n",
    "        if self.verbose:\n",
    "            ftrans = open(self.config['val_set_out'], 'w')\n",
    "\n",
    "        for i, line in enumerate(self.data_stream.get_epoch_iterator()):\n",
    "            \"\"\"\n",
    "            Load the sentence, retrieve the sample, write to file\n",
    "            \"\"\"\n",
    "\n",
    "            seq = self._oov_to_unk(\n",
    "                line[0], self.config['src_vocab_size'], self.unk_idx)\n",
    "            initial_state_context = line[-1]\n",
    "            \n",
    "            input_ = numpy.tile(seq, (self.config['beam_size'], 1))\n",
    "            context_input_ = numpy.tile(initial_state_context, (self.config['beam_size'], 1))\n",
    "\n",
    "            # draw sample, checking to ensure we don't get an empty string back\n",
    "            # beam search param names come from WHERE??\n",
    "            trans, costs = \\\n",
    "                self.beam_search.search(\n",
    "                    input_values={self.source_sentence: input_,\n",
    "                                  self.initial_context: context_input_},\n",
    "                    max_length=3*len(seq), eol_symbol=self.eos_idx,\n",
    "                    ignore_first_eol=True)\n",
    "\n",
    "            # normalize costs according to the sequence lengths\n",
    "            if self.normalize:\n",
    "                lengths = numpy.array([len(s) for s in trans])\n",
    "                costs = costs / lengths\n",
    "\n",
    "            nbest_idx = numpy.argsort(costs)[:self.n_best]\n",
    "            for j, best in enumerate(nbest_idx):\n",
    "                try:\n",
    "                    total_cost += costs[best]\n",
    "                    trans_out = trans[best]\n",
    "\n",
    "                    # convert idx to words\n",
    "                    trans_out = self._idx_to_word(trans_out, self.trg_ivocab)\n",
    "\n",
    "                except ValueError:\n",
    "                    logger.info(\n",
    "                        \"Can NOT find a translation for line: {}\".format(i+1))\n",
    "                    trans_out = '<UNK>'\n",
    "\n",
    "                if j == 0:\n",
    "                    # Write to subprocess and file if it exists\n",
    "                    print(trans_out, file=mb_subprocess.stdin)\n",
    "                    if self.verbose:\n",
    "                        print(trans_out, file=ftrans)\n",
    "\n",
    "            if i != 0 and i % 100 == 0:\n",
    "                logger.info(\n",
    "                    \"Translated {} lines of validation set...\".format(i))\n",
    "\n",
    "            mb_subprocess.stdin.flush()\n",
    "\n",
    "        logger.info(\"Total cost of the validation: {}\".format(total_cost))\n",
    "        self.data_stream.reset()\n",
    "        if self.verbose:\n",
    "            ftrans.close()\n",
    "\n",
    "        # send end of file, read output.\n",
    "        mb_subprocess.stdin.close()\n",
    "        stdout = mb_subprocess.stdout.readline()\n",
    "        logger.info(stdout)\n",
    "        out_parse = re.match(r'BLEU = [-.0-9]+', stdout)\n",
    "        logger.info(\"Validation Took: {} minutes\".format(\n",
    "            float(time.time() - val_start_time) / 60.))\n",
    "        assert out_parse is not None\n",
    "\n",
    "        # extract the score\n",
    "        bleu_score = float(out_parse.group()[6:])\n",
    "        self.val_bleu_curve.append(bleu_score)\n",
    "        logger.info(bleu_score)\n",
    "        mb_subprocess.terminate()\n",
    "\n",
    "\n",
    "        return bleu_score\n",
    "\n",
    "    def _is_valid_to_save(self, bleu_score):\n",
    "        if not self.best_models or min(self.best_models,\n",
    "           key=operator.attrgetter('bleu_score')).bleu_score < bleu_score:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _save_model(self, bleu_score):\n",
    "        if self._is_valid_to_save(bleu_score):\n",
    "            model = ModelInfo(bleu_score, self.config['saveto'])\n",
    "\n",
    "            # Manage n-best model list first\n",
    "            if len(self.best_models) >= self.track_n_models:\n",
    "                old_model = self.best_models[0]\n",
    "                if old_model.path and os.path.isfile(old_model.path):\n",
    "                    logger.info(\"Deleting old model %s\" % old_model.path)\n",
    "                    os.remove(old_model.path)\n",
    "                self.best_models.remove(old_model)\n",
    "\n",
    "            self.best_models.append(model)\n",
    "            self.best_models.sort(key=operator.attrgetter('bleu_score'))\n",
    "\n",
    "            # Save the model here\n",
    "            s = signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
    "            logger.info(\"Saving new model {}\".format(model.path))\n",
    "\n",
    "            SaveLoadUtils.save_parameter_values(self.main_loop.model.get_parameter_values(), model.path)\n",
    "            numpy.savez(\n",
    "                os.path.join(self.config['saveto'], 'val_bleu_scores.npz'),\n",
    "                bleu_scores=self.val_bleu_curve)\n",
    "            signal.signal(signal.SIGINT, s)\n",
    "\n",
    "\n",
    "class ModelInfo:\n",
    "    \"\"\"Utility class to keep track of evaluated models.\"\"\"\n",
    "\n",
    "    def __init__(self, bleu_score, path=None):\n",
    "        self.bleu_score = bleu_score\n",
    "        self.path = self._generate_path(path)\n",
    "\n",
    "    def _generate_path(self, path):\n",
    "        gen_path = os.path.join(\n",
    "            path, 'best_bleu_model_%d_BLEU%.2f.npz' %\n",
    "            (int(time.time()), self.bleu_score) if path else None)\n",
    "        return gen_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from theano import tensor\n",
    "from toolz import merge\n",
    "import numpy\n",
    "import pickle\n",
    "from subprocess import Popen, PIPE\n",
    "import codecs\n",
    "\n",
    "from blocks.algorithms import (GradientDescent, StepClipping,\n",
    "                               CompositeRule, Adam, AdaDelta)\n",
    "from blocks.extensions import FinishAfter, Printing, Timing\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.graph import ComputationGraph, apply_noise, apply_dropout\n",
    "from blocks.initialization import IsotropicGaussian, Orthogonal, Constant\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.select import Selector\n",
    "from blocks.search import BeamSearch\n",
    "from blocks_extras.extensions.plot import Plot\n",
    "\n",
    "from machine_translation.checkpoint import CheckpointNMT, LoadNMT\n",
    "from machine_translation.model import BidirectionalEncoder, Decoder\n",
    "# we reimplement sampling for the context NMT\n",
    "# from machine_translation.sampling import BleuValidator, Sampler, SamplingBase\n",
    "from machine_translation.stream import (get_tr_stream, get_dev_stream,\n",
    "                                        _ensure_special_tokens)\n",
    "\n",
    "try:\n",
    "    from blocks_extras.extensions.plot import Plot\n",
    "    BOKEH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    BOKEH_AVAILABLE = False\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def main(config, tr_stream, dev_stream, use_bokeh=False):\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    source_sentence = tensor.lmatrix('source')\n",
    "    source_sentence_mask = tensor.matrix('source_mask')\n",
    "    target_sentence = tensor.lmatrix('target')\n",
    "    target_sentence_mask = tensor.matrix('target_mask')\n",
    "    initial_context = tensor.matrix('initial_context')\n",
    "    \n",
    "\n",
    "    \n",
    "    # Construct model\n",
    "    logger.info('Building RNN encoder-decoder')\n",
    "    encoder = BidirectionalEncoder(\n",
    "        config['src_vocab_size'], config['enc_embed'], config['enc_nhids'])\n",
    "\n",
    "    decoder = InitialContextDecoder(\n",
    "        config['trg_vocab_size'], config['dec_embed'], config['dec_nhids'],\n",
    "        config['enc_nhids'] * 2, config['context_dim'])\n",
    "    \n",
    "    cost = decoder.cost(\n",
    "        encoder.apply(source_sentence, source_sentence_mask),\n",
    "        source_sentence_mask, target_sentence, target_sentence_mask, initial_context)\n",
    "\n",
    "    logger.info('Creating computational graph')\n",
    "    cg = ComputationGraph(cost)\n",
    "\n",
    "    # Initialize model\n",
    "    logger.info('Initializing model')\n",
    "    encoder.weights_init = decoder.weights_init = IsotropicGaussian(\n",
    "        config['weight_scale'])\n",
    "    encoder.biases_init = decoder.biases_init = Constant(0)\n",
    "    encoder.push_initialization_config()\n",
    "    decoder.push_initialization_config()\n",
    "    encoder.bidir.prototype.weights_init = Orthogonal()\n",
    "    decoder.transition.weights_init = Orthogonal()\n",
    "    encoder.initialize()\n",
    "    decoder.initialize()\n",
    "\n",
    "    # apply dropout for regularization\n",
    "    if config['dropout'] < 1.0:\n",
    "        # dropout is applied to the output of maxout in ghog\n",
    "        # this is the probability of dropping out, so you probably want to make it <=0.5\n",
    "        logger.info('Applying dropout')\n",
    "        dropout_inputs = [x for x in cg.intermediary_variables\n",
    "                          if x.name == 'maxout_apply_output']\n",
    "        cg = apply_dropout(cg, dropout_inputs, config['dropout'])\n",
    "\n",
    "    # Apply weight noise for regularization\n",
    "    if config['weight_noise_ff'] > 0.0:\n",
    "        logger.info('Applying weight noise to ff layers')\n",
    "        enc_params = Selector(encoder.lookup).get_parameters().values()\n",
    "        enc_params += Selector(encoder.fwd_fork).get_parameters().values()\n",
    "        enc_params += Selector(encoder.back_fork).get_parameters().values()\n",
    "        dec_params = Selector(\n",
    "            decoder.sequence_generator.readout).get_parameters().values()\n",
    "        dec_params += Selector(\n",
    "            decoder.sequence_generator.fork).get_parameters().values()\n",
    "        dec_params += Selector(decoder.transition.initial_transformer).get_parameters().values()\n",
    "        cg = apply_noise(cg, enc_params+dec_params, config['weight_noise_ff'])\n",
    "\n",
    "    # TODO: weight noise for recurrent params isn't currently implemented -- see config['weight_noise_rec']\n",
    "    # Print shapes\n",
    "    shapes = [param.get_value().shape for param in cg.parameters]\n",
    "    logger.info(\"Parameter shapes: \")\n",
    "    for shape, count in Counter(shapes).most_common():\n",
    "        logger.info('    {:15}: {}'.format(shape, count))\n",
    "    logger.info(\"Total number of parameters: {}\".format(len(shapes)))\n",
    "\n",
    "    # Print parameter names\n",
    "    enc_dec_param_dict = merge(Selector(encoder).get_parameters(),\n",
    "                               Selector(decoder).get_parameters())\n",
    "    logger.info(\"Parameter names: \")\n",
    "    for name, value in enc_dec_param_dict.items():\n",
    "        logger.info('    {:15}: {}'.format(value.get_value().shape, name))\n",
    "    logger.info(\"Total number of parameters: {}\"\n",
    "                .format(len(enc_dec_param_dict)))\n",
    "\n",
    "    # Set up training model\n",
    "    logger.info(\"Building model\")\n",
    "    training_model = Model(cost)\n",
    "\n",
    "    # create the training directory, and copy this config there if directory doesn't exist\n",
    "    if not os.path.isdir(config['saveto']):\n",
    "        os.makedirs(config['saveto'])\n",
    "        shutil.copy(config['config_file'], config['saveto'])\n",
    "\n",
    "    # Set extensions\n",
    "    logger.info(\"Initializing extensions\")\n",
    "    extensions = [\n",
    "        FinishAfter(after_n_batches=config['finish_after']),\n",
    "        TrainingDataMonitoring([cost], after_batch=True),\n",
    "        Printing(after_batch=True),\n",
    "        CheckpointNMT(config['saveto'],\n",
    "                      every_n_batches=config['save_freq'])\n",
    "    ]\n",
    "\n",
    "    # Create the theano variables that we need for the sampling graph\n",
    "    sampling_input = tensor.lmatrix('input')\n",
    "    sampling_context = tensor.matrix('context_input')\n",
    "    \n",
    "    # WORKING: change this part to account for the new initial context for decoder\n",
    "    # Set up beam search and sampling computation graphs if necessary\n",
    "    if config['hook_samples'] >= 1 or config['bleu_script'] is not None:\n",
    "        logger.info(\"Building sampling model\")\n",
    "        sampling_representation = encoder.apply(\n",
    "            sampling_input, tensor.ones(sampling_input.shape))\n",
    "        \n",
    "        # TODO: decoder generate function also needs to include the new initial contexts in the kwargs\n",
    "        generated = decoder.generate(sampling_input, sampling_representation, sampling_context)\n",
    "        search_model = Model(generated)\n",
    "        _, samples = VariableFilter(\n",
    "            bricks=[decoder.sequence_generator], name=\"outputs\")(\n",
    "                ComputationGraph(generated[1]))  # generated[1] is next_outputs\n",
    "\n",
    "    # Add sampling\n",
    "    # TODO: currently commented because we need to modify the sampler to use the contexts\n",
    "    if config['hook_samples'] >= 1:\n",
    "        logger.info(\"Building sampler\")\n",
    "        extensions.append(\n",
    "            Sampler(model=search_model, data_stream=tr_stream,\n",
    "                    hook_samples=config['hook_samples'],\n",
    "                    every_n_batches=config['sampling_freq'],\n",
    "                    src_vocab=source_vocab,\n",
    "                    trg_vocab=target_vocab,\n",
    "                    src_vocab_size=config['src_vocab_size'],\n",
    "                   ))\n",
    "\n",
    "\n",
    "    # TODO: add sampling_context to BleuValidator and Sampler\n",
    "    # Add early stopping based on bleu\n",
    "    if config['bleu_script'] is not None:\n",
    "        logger.info(\"Building bleu validator\")\n",
    "        extensions.append(\n",
    "            BleuValidator(sampling_input, sampling_context, samples=samples, config=config,\n",
    "                          model=search_model, data_stream=dev_stream,\n",
    "                          src_vocab=source_vocab,\n",
    "                          trg_vocab=target_vocab,\n",
    "                          normalize=config['normalized_bleu'],\n",
    "                          every_n_batches=config['bleu_val_freq']))\n",
    "\n",
    "    # Reload model if necessary\n",
    "    if config['reload']:\n",
    "        extensions.append(LoadNMT(config['saveto']))\n",
    "\n",
    "    # Plot cost in bokeh if necessary\n",
    "    if use_bokeh and BOKEH_AVAILABLE:\n",
    "        extensions.append(\n",
    "            Plot(config['model_save_directory'], channels=[['decoder_cost_cost'], ['validation_set_bleu_score']],\n",
    "                 every_n_batches=10))\n",
    "\n",
    "    # Set up training algorithm\n",
    "    logger.info(\"Initializing training algorithm\")\n",
    "    # if there is dropout or random noise, we need to use the output of the modified graph\n",
    "    if config['dropout'] < 1.0 or config['weight_noise_ff'] > 0.0:\n",
    "        algorithm = GradientDescent(\n",
    "            cost=cg.outputs[0], parameters=cg.parameters,\n",
    "            step_rule=CompositeRule([StepClipping(config['step_clipping']),\n",
    "                                     eval(config['step_rule'])()])\n",
    "        )\n",
    "    else:\n",
    "        algorithm = GradientDescent(\n",
    "            cost=cost, parameters=cg.parameters,\n",
    "            step_rule=CompositeRule([StepClipping(config['step_clipping']),\n",
    "                                     eval(config['step_rule'])()])\n",
    "        )\n",
    "\n",
    "    # enrich the logged information\n",
    "    extensions.append(\n",
    "        Timing(every_n_batches=100)\n",
    "    )\n",
    "\n",
    "    # Initialize main loop\n",
    "    logger.info(\"Initializing main loop\")\n",
    "    main_loop = MainLoop(\n",
    "        model=training_model,\n",
    "        algorithm=algorithm,\n",
    "        data_stream=tr_stream,\n",
    "        extensions=extensions\n",
    "    )\n",
    "\n",
    "    # Train!\n",
    "    main_loop.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating theano variables\n",
      "INFO:__main__:Building RNN encoder-decoder\n",
      "WARNING:blocks.bricks.recurrent:unknown input att_trans_do_apply_initial_state_context\n",
      "\n",
      "Your function uses a non-shared variable other than those given by scan explicitly. That can significantly slow down `tensor.grad` call. Did you forget to declare it in `contexts`?\n",
      "INFO:__main__:Creating computational graph\n",
      "INFO:__main__:Initializing model\n",
      "INFO:__main__:Applying dropout\n",
      "INFO:__main__:Parameter shapes: \n",
      "INFO:__main__:    (800,)         : 8\n",
      "INFO:__main__:    (800, 800)     : 5\n",
      "INFO:__main__:    (300, 800)     : 4\n",
      "INFO:__main__:    (800, 1600)    : 3\n",
      "INFO:__main__:    (300, 1600)    : 3\n",
      "INFO:__main__:    (1600, 800)    : 3\n",
      "INFO:__main__:    (1600,)        : 3\n",
      "INFO:__main__:    (20000, 300)   : 2\n",
      "INFO:__main__:    (20000,)       : 1\n",
      "INFO:__main__:    (800, 1)       : 1\n",
      "INFO:__main__:    (1000, 500)    : 1\n",
      "INFO:__main__:    (300, 20000)   : 1\n",
      "INFO:__main__:    (4896, 1000)   : 1\n",
      "INFO:__main__:    (1000,)        : 1\n",
      "INFO:__main__:    (1600, 1600)   : 1\n",
      "INFO:__main__:    (500, 800)     : 1\n",
      "INFO:__main__:    (500,)         : 1\n",
      "INFO:__main__:    (400, 300)     : 1\n",
      "INFO:__main__:Total number of parameters: 41\n",
      "INFO:__main__:Parameter names: \n",
      "INFO:__main__:    (800, 800)     : /initialcontextdecoder/sequencegenerator/readout/merge/transform_states.W\n",
      "INFO:__main__:    (500,)         : /initialcontextdecoder/sequencegenerator/att_trans/decoder/state_initializer/linear_1.b\n",
      "INFO:__main__:    (1600,)        : /initialcontextdecoder/sequencegenerator/fork/fork_gate_inputs.b\n",
      "INFO:__main__:    (800, 1600)    : /bidirectionalencoder/bidirectionalwmt15/backward.state_to_gates\n",
      "INFO:__main__:    (300, 1600)    : /bidirectionalencoder/fwd_fork/fork_gate_inputs.W\n",
      "INFO:__main__:    (1600, 800)    : /initialcontextdecoder/sequencegenerator/att_trans/distribute/fork_inputs.W\n",
      "INFO:__main__:    (800,)         : /bidirectionalencoder/back_fork/fork_inputs.b\n",
      "INFO:__main__:    (20000, 300)   : /bidirectionalencoder/embeddings.W\n",
      "INFO:__main__:    (300, 800)     : /initialcontextdecoder/sequencegenerator/fork/fork_inputs.W\n",
      "INFO:__main__:    (800,)         : /initialcontextdecoder/sequencegenerator/fork/fork_inputs.b\n",
      "INFO:__main__:    (300, 800)     : /bidirectionalencoder/back_fork/fork_inputs.W\n",
      "INFO:__main__:    (800, 1)       : /initialcontextdecoder/sequencegenerator/att_trans/attention/energy_comp/linear.W\n",
      "INFO:__main__:    (800, 800)     : /bidirectionalencoder/bidirectionalwmt15/backward.state_to_state\n",
      "INFO:__main__:    (1600,)        : /bidirectionalencoder/fwd_fork/fork_gate_inputs.b\n",
      "INFO:__main__:    (1000, 500)    : /initialcontextdecoder/sequencegenerator/att_trans/decoder/state_initializer/linear_1.W\n",
      "INFO:__main__:    (1600, 800)    : /initialcontextdecoder/sequencegenerator/readout/merge/transform_weighted_averages.W\n",
      "INFO:__main__:    (4896, 1000)   : /initialcontextdecoder/sequencegenerator/att_trans/decoder/state_initializer/linear_0.W\n",
      "INFO:__main__:    (800, 800)     : /bidirectionalencoder/bidirectionalwmt15/forward.state_to_state\n",
      "INFO:__main__:    (800,)         : /bidirectionalencoder/bidirectionalwmt15/backward.initial_state\n",
      "INFO:__main__:    (800, 1600)    : /initialcontextdecoder/sequencegenerator/att_trans/decoder.state_to_gates\n",
      "INFO:__main__:    (800, 1600)    : /bidirectionalencoder/bidirectionalwmt15/forward.state_to_gates\n",
      "INFO:__main__:    (800, 800)     : /initialcontextdecoder/sequencegenerator/att_trans/decoder.state_to_state\n",
      "INFO:__main__:    (800,)         : /initialcontextdecoder/sequencegenerator/att_trans/decoder/state_initializer/linear_2.b\n",
      "INFO:__main__:    (300, 800)     : /bidirectionalencoder/fwd_fork/fork_inputs.W\n",
      "INFO:__main__:    (800,)         : /bidirectionalencoder/bidirectionalwmt15/forward.initial_state\n",
      "INFO:__main__:    (1600,)        : /bidirectionalencoder/back_fork/fork_gate_inputs.b\n",
      "INFO:__main__:    (800,)         : /initialcontextdecoder/sequencegenerator/att_trans/attention/preprocess.b\n",
      "INFO:__main__:    (20000,)       : /initialcontextdecoder/sequencegenerator/readout/initializablefeedforwardsequence/softmax1.b\n",
      "INFO:__main__:    (1600, 1600)   : /initialcontextdecoder/sequencegenerator/att_trans/distribute/fork_gate_inputs.W\n",
      "INFO:__main__:    (300, 1600)    : /initialcontextdecoder/sequencegenerator/fork/fork_gate_inputs.W\n",
      "INFO:__main__:    (1000,)        : /initialcontextdecoder/sequencegenerator/att_trans/decoder/state_initializer/linear_0.b\n",
      "INFO:__main__:    (400, 300)     : /initialcontextdecoder/sequencegenerator/readout/initializablefeedforwardsequence/softmax0.W\n",
      "INFO:__main__:    (300, 20000)   : /initialcontextdecoder/sequencegenerator/readout/initializablefeedforwardsequence/softmax1.W\n",
      "INFO:__main__:    (800, 800)     : /initialcontextdecoder/sequencegenerator/att_trans/attention/state_trans/transform_states.W\n",
      "INFO:__main__:    (800,)         : /initialcontextdecoder/sequencegenerator/readout/initializablefeedforwardsequence/maxout_bias.b\n",
      "INFO:__main__:    (20000, 300)   : /initialcontextdecoder/sequencegenerator/readout/lookupfeedbackwmt15/lookuptable.W\n",
      "INFO:__main__:    (300, 1600)    : /bidirectionalencoder/back_fork/fork_gate_inputs.W\n",
      "INFO:__main__:    (1600, 800)    : /initialcontextdecoder/sequencegenerator/att_trans/attention/preprocess.W\n",
      "INFO:__main__:    (300, 800)     : /initialcontextdecoder/sequencegenerator/readout/merge/transform_feedback.W\n",
      "INFO:__main__:    (800,)         : /bidirectionalencoder/fwd_fork/fork_inputs.b\n",
      "INFO:__main__:    (500, 800)     : /initialcontextdecoder/sequencegenerator/att_trans/decoder/state_initializer/linear_2.W\n",
      "INFO:__main__:Total number of parameters: 41\n",
      "INFO:__main__:Building model\n",
      "INFO:__main__:Initializing extensions\n",
      "INFO:__main__:Building sampling model\n",
      "WARNING:blocks.bricks.recurrent:unknown input sequencegenerator_generate_initial_state_context\n",
      "\n",
      "Your function uses a non-shared variable other than those given by scan explicitly. That can significantly slow down `tensor.grad` call. Did you forget to declare it in `contexts`?\n",
      "INFO:__main__:Building sampler\n",
      "INFO:__main__:Building bleu validator\n",
      "INFO:__main__:Initializing training algorithm\n",
      "INFO:__main__:Initializing main loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved session configuration for http://localhost:5006/\n",
      "To override, pass 'load_from_config=False' to Session\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 1\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 1:\n",
      "\t initialcontextdecoder_cost_cost: 95.816368103\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 2\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 2:\n",
      "\t initialcontextdecoder_cost_cost: 102.966468811\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 3\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 3:\n",
      "\t initialcontextdecoder_cost_cost: 111.838180542\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 4\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 4:\n",
      "\t initialcontextdecoder_cost_cost: 118.728370667\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 5\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 5:\n",
      "\t initialcontextdecoder_cost_cost: 130.313262939\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 6\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 6:\n",
      "\t initialcontextdecoder_cost_cost: 141.625366211\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 7\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 7:\n",
      "\t initialcontextdecoder_cost_cost: 153.424942017\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 8\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 8:\n",
      "\t initialcontextdecoder_cost_cost: 176.054580688\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 9\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 9:\n",
      "\t initialcontextdecoder_cost_cost: 216.66885376\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 10\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 10:\n",
      "\t initialcontextdecoder_cost_cost: 81.989440918\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 11\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 11:\n",
      "\t initialcontextdecoder_cost_cost: 94.8744659424\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 12\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 12:\n",
      "\t initialcontextdecoder_cost_cost: 100.879821777\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 13\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 13:\n",
      "\t initialcontextdecoder_cost_cost: 105.746459961\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 14\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 14:\n",
      "\t initialcontextdecoder_cost_cost: 99.9709625244\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 15\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 15:\n",
      "\t initialcontextdecoder_cost_cost: 140.844924927\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 16\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 16:\n",
      "\t initialcontextdecoder_cost_cost: 117.425949097\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 17\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 17:\n",
      "\t initialcontextdecoder_cost_cost: 158.190078735\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 18\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 18:\n",
      "\t initialcontextdecoder_cost_cost: 139.606735229\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 19\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 19:\n",
      "\t initialcontextdecoder_cost_cost: 229.811553955\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 20\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 20:\n",
      "\t initialcontextdecoder_cost_cost: 60.3527603149\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 21\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 21:\n",
      "\t initialcontextdecoder_cost_cost: 77.7583389282\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 22\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 22:\n",
      "\t initialcontextdecoder_cost_cost: 83.4535980225\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 23\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 23:\n",
      "\t initialcontextdecoder_cost_cost: 93.9513778687\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 24\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 24:\n",
      "\t initialcontextdecoder_cost_cost: 90.0352020264\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 25\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 25:\n",
      "\t initialcontextdecoder_cost_cost: 122.556228638\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 26\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 26:\n",
      "\t initialcontextdecoder_cost_cost: 101.598922729\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 27\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 27:\n",
      "\t initialcontextdecoder_cost_cost: 152.877288818\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 28\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 28:\n",
      "\t initialcontextdecoder_cost_cost: 126.455062866\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 29\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 29:\n",
      "\t initialcontextdecoder_cost_cost: 205.279006958\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 30\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 30:\n",
      "\t initialcontextdecoder_cost_cost: 50.4693260193\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 31\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 31:\n",
      "\t initialcontextdecoder_cost_cost: 64.4431533813\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 32\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 32:\n",
      "\t initialcontextdecoder_cost_cost: 75.1314468384\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 33\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 33:\n",
      "\t initialcontextdecoder_cost_cost: 85.9373550415\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 34\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 34:\n",
      "\t initialcontextdecoder_cost_cost: 84.0230865479\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 35\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 35:\n",
      "\t initialcontextdecoder_cost_cost: 105.366592407\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 36\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 36:\n",
      "\t initialcontextdecoder_cost_cost: 91.7425537109\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 37\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 37:\n",
      "\t initialcontextdecoder_cost_cost: 105.693603516\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 38\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 38:\n",
      "\t initialcontextdecoder_cost_cost: 122.206031799\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 39\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 39:\n",
      "\t initialcontextdecoder_cost_cost: 175.053146362\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 40\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 40:\n",
      "\t initialcontextdecoder_cost_cost: 52.169342041\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 41\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 41:\n",
      "\t initialcontextdecoder_cost_cost: 69.9825286865\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 42\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 42:\n",
      "\t initialcontextdecoder_cost_cost: 72.290725708\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 43\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 43:\n",
      "\t initialcontextdecoder_cost_cost: 87.3284072876\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 44\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 44:\n",
      "\t initialcontextdecoder_cost_cost: 79.9727096558\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 45\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 45:\n",
      "\t initialcontextdecoder_cost_cost: 84.6495132446\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 46\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 46:\n",
      "\t initialcontextdecoder_cost_cost: 110.142990112\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 47\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 47:\n",
      "\t initialcontextdecoder_cost_cost: 102.846412659\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 48\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 48:\n",
      "\t initialcontextdecoder_cost_cost: 126.474586487\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 49\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 49:\n",
      "\t initialcontextdecoder_cost_cost: 159.543914795\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 50\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 50:\n",
      "\t initialcontextdecoder_cost_cost: 58.8676528931\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 51\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 51:\n",
      "\t initialcontextdecoder_cost_cost: 60.159740448\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 52\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 52:\n",
      "\t initialcontextdecoder_cost_cost: 67.6298675537\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 53\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 53:\n",
      "\t initialcontextdecoder_cost_cost: 73.4352798462\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 54\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 54:\n",
      "\t initialcontextdecoder_cost_cost: 88.4106063843\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 55\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 55:\n",
      "\t initialcontextdecoder_cost_cost: 83.0039825439\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 56\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 56:\n",
      "\t initialcontextdecoder_cost_cost: 110.402664185\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 57\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 57:\n",
      "\t initialcontextdecoder_cost_cost: 99.8869781494\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 58\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 58:\n",
      "\t initialcontextdecoder_cost_cost: 113.092163086\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 59\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 59:\n",
      "\t initialcontextdecoder_cost_cost: 145.149093628\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 60\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 60:\n",
      "\t initialcontextdecoder_cost_cost: 57.5833015442\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 61\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 61:\n",
      "\t initialcontextdecoder_cost_cost: 56.3117904663\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 62\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 62:\n",
      "\t initialcontextdecoder_cost_cost: 67.392791748\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 63\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 63:\n",
      "\t initialcontextdecoder_cost_cost: 74.1914901733\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 64\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 64:\n",
      "\t initialcontextdecoder_cost_cost: 78.0043182373\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 65\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 65:\n",
      "\t initialcontextdecoder_cost_cost: 90.5019073486\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 66\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 66:\n",
      "\t initialcontextdecoder_cost_cost: 92.4526367188\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 67\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 67:\n",
      "\t initialcontextdecoder_cost_cost: 108.958068848\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 68\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 68:\n",
      "\t initialcontextdecoder_cost_cost: 116.70475769\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 69\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 69:\n",
      "\t initialcontextdecoder_cost_cost: 158.609542847\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 70\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 70:\n",
      "\t initialcontextdecoder_cost_cost: 51.4458389282\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 71\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 71:\n",
      "\t initialcontextdecoder_cost_cost: 59.3632621765\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 72\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 72:\n",
      "\t initialcontextdecoder_cost_cost: 66.4953231812\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 73\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 73:\n",
      "\t initialcontextdecoder_cost_cost: 77.467666626\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 74\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 74:\n",
      "\t initialcontextdecoder_cost_cost: 73.5288543701\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 75\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 75:\n",
      "\t initialcontextdecoder_cost_cost: 85.1128463745\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 76\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 76:\n",
      "\t initialcontextdecoder_cost_cost: 90.3677139282\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 77\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 77:\n",
      "\t initialcontextdecoder_cost_cost: 117.015151978\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 78\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 78:\n",
      "\t initialcontextdecoder_cost_cost: 110.765403748\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 79\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 79:\n",
      "\t initialcontextdecoder_cost_cost: 148.961303711\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 80\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 80:\n",
      "\t initialcontextdecoder_cost_cost: 50.4492607117\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 81\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 81:\n",
      "\t initialcontextdecoder_cost_cost: 52.1487197876\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 82\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 82:\n",
      "\t initialcontextdecoder_cost_cost: 67.0267486572\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 83\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 83:\n",
      "\t initialcontextdecoder_cost_cost: 68.9178924561\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 84\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 84:\n",
      "\t initialcontextdecoder_cost_cost: 87.3617172241\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 85\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 85:\n",
      "\t initialcontextdecoder_cost_cost: 80.1373443604\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 86\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 86:\n",
      "\t initialcontextdecoder_cost_cost: 87.7685165405\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 87\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 87:\n",
      "\t initialcontextdecoder_cost_cost: 94.7069778442\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 88\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 88:\n",
      "\t initialcontextdecoder_cost_cost: 121.526832581\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 89\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 89:\n",
      "\t initialcontextdecoder_cost_cost: 148.329055786\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 90\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 90:\n",
      "\t initialcontextdecoder_cost_cost: 52.3520698547\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 91\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 91:\n",
      "\t initialcontextdecoder_cost_cost: 53.0798950195\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 92\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 92:\n",
      "\t initialcontextdecoder_cost_cost: 73.4535446167\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 93\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 93:\n",
      "\t initialcontextdecoder_cost_cost: 63.828742981\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 94\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 94:\n",
      "\t initialcontextdecoder_cost_cost: 70.5662384033\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 95\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 95:\n",
      "\t initialcontextdecoder_cost_cost: 76.6778717041\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 96\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 96:\n",
      "\t initialcontextdecoder_cost_cost: 92.6788330078\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 97\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 97:\n",
      "\t initialcontextdecoder_cost_cost: 90.3911590576\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 98\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 98:\n",
      "\t initialcontextdecoder_cost_cost: 111.036422729\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 99\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 99:\n",
      "\t initialcontextdecoder_cost_cost: 128.699645996\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 100\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 100:\n",
      "\t initialcontextdecoder_cost_cost: 45.5508804321\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 101\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 101:\n",
      "\t initialcontextdecoder_cost_cost: 52.3269424438\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 102\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 102:\n",
      "\t initialcontextdecoder_cost_cost: 60.5637931824\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 103\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 103:\n",
      "\t initialcontextdecoder_cost_cost: 62.8031387329\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 104\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 104:\n",
      "\t initialcontextdecoder_cost_cost: 71.5157318115\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 105\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 105:\n",
      "\t initialcontextdecoder_cost_cost: 88.8107223511\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 106\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 106:\n",
      "\t initialcontextdecoder_cost_cost: 83.7419052124\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 107\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 107:\n",
      "\t initialcontextdecoder_cost_cost: 106.394554138\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 108\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 108:\n",
      "\t initialcontextdecoder_cost_cost: 102.76335144\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 109\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 109:\n",
      "\t initialcontextdecoder_cost_cost: 153.975799561\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 110\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 110:\n",
      "\t initialcontextdecoder_cost_cost: 46.6817550659\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 111\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 111:\n",
      "\t initialcontextdecoder_cost_cost: 53.4290161133\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 112\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 112:\n",
      "\t initialcontextdecoder_cost_cost: 68.3684234619\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 113\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 113:\n",
      "\t initialcontextdecoder_cost_cost: 62.9200439453\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 114\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 114:\n",
      "\t initialcontextdecoder_cost_cost: 77.4380645752\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 115\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 115:\n",
      "\t initialcontextdecoder_cost_cost: 74.2686309814\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 116\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 116:\n",
      "\t initialcontextdecoder_cost_cost: 88.3602218628\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 117\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 117:\n",
      "\t initialcontextdecoder_cost_cost: 91.8210830688\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 118\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 118:\n",
      "\t initialcontextdecoder_cost_cost: 112.099411011\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 119\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 119:\n",
      "\t initialcontextdecoder_cost_cost: 131.324645996\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 120\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 120:\n",
      "\t initialcontextdecoder_cost_cost: 45.7460861206\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 121\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 121:\n",
      "\t initialcontextdecoder_cost_cost: 49.7985458374\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 122\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 122:\n",
      "\t initialcontextdecoder_cost_cost: 59.0639419556\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 123\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 123:\n",
      "\t initialcontextdecoder_cost_cost: 70.4955291748\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 124\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 124:\n",
      "\t initialcontextdecoder_cost_cost: 69.1615219116\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 125\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 125:\n",
      "\t initialcontextdecoder_cost_cost: 80.6820220947\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 126\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 126:\n",
      "\t initialcontextdecoder_cost_cost: 79.7008514404\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 127\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 127:\n",
      "\t initialcontextdecoder_cost_cost: 90.0510787964\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 128\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 128:\n",
      "\t initialcontextdecoder_cost_cost: 101.257072449\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 129\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 129:\n",
      "\t initialcontextdecoder_cost_cost: 156.430389404\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 130\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 130:\n",
      "\t initialcontextdecoder_cost_cost: 47.1481285095\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 131\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 131:\n",
      "\t initialcontextdecoder_cost_cost: 52.449256897\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 132\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 132:\n",
      "\t initialcontextdecoder_cost_cost: 57.0787734985\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 133\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 133:\n",
      "\t initialcontextdecoder_cost_cost: 61.8988113403\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 134\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 134:\n",
      "\t initialcontextdecoder_cost_cost: 71.7840957642\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 135\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 135:\n",
      "\t initialcontextdecoder_cost_cost: 77.2658538818\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 136\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 136:\n",
      "\t initialcontextdecoder_cost_cost: 89.2331390381\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 137\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 137:\n",
      "\t initialcontextdecoder_cost_cost: 90.8796005249\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 138\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 138:\n",
      "\t initialcontextdecoder_cost_cost: 109.067970276\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 139\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 139:\n",
      "\t initialcontextdecoder_cost_cost: 126.391990662\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 140\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 140:\n",
      "\t initialcontextdecoder_cost_cost: 49.3803939819\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 141\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 141:\n",
      "\t initialcontextdecoder_cost_cost: 49.9621391296\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 142\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 142:\n",
      "\t initialcontextdecoder_cost_cost: 60.3200492859\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 143\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 143:\n",
      "\t initialcontextdecoder_cost_cost: 62.0833511353\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 144\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 144:\n",
      "\t initialcontextdecoder_cost_cost: 69.9699935913\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 145\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 145:\n",
      "\t initialcontextdecoder_cost_cost: 79.6862030029\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 146\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 146:\n",
      "\t initialcontextdecoder_cost_cost: 79.7897109985\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 147\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 147:\n",
      "\t initialcontextdecoder_cost_cost: 95.602394104\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 148\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 148:\n",
      "\t initialcontextdecoder_cost_cost: 97.4460220337\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 149\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 149:\n",
      "\t initialcontextdecoder_cost_cost: 134.087005615\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 150\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 150:\n",
      "\t initialcontextdecoder_cost_cost: 52.9413528442\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 151\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 151:\n",
      "\t initialcontextdecoder_cost_cost: 55.1996879578\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 152\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 152:\n",
      "\t initialcontextdecoder_cost_cost: 63.8590087891\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 153\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 153:\n",
      "\t initialcontextdecoder_cost_cost: 67.1618423462\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 154\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 154:\n",
      "\t initialcontextdecoder_cost_cost: 73.4649200439\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 155\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 155:\n",
      "\t initialcontextdecoder_cost_cost: 77.5247802734\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 156\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 156:\n",
      "\t initialcontextdecoder_cost_cost: 85.5536346436\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 157\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 157:\n",
      "\t initialcontextdecoder_cost_cost: 88.087097168\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 158\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 158:\n",
      "\t initialcontextdecoder_cost_cost: 114.245620728\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 159\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 159:\n",
      "\t initialcontextdecoder_cost_cost: 134.158111572\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 160\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 160:\n",
      "\t initialcontextdecoder_cost_cost: 48.8317985535\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 161\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 161:\n",
      "\t initialcontextdecoder_cost_cost: 50.3335609436\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 162\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 162:\n",
      "\t initialcontextdecoder_cost_cost: 55.9228134155\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 163\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 163:\n",
      "\t initialcontextdecoder_cost_cost: 61.5460891724\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 164\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 164:\n",
      "\t initialcontextdecoder_cost_cost: 65.6404342651\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 165\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 165:\n",
      "\t initialcontextdecoder_cost_cost: 75.3343963623\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 166\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 166:\n",
      "\t initialcontextdecoder_cost_cost: 80.0013427734\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 167\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 167:\n",
      "\t initialcontextdecoder_cost_cost: 89.6600112915\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 168\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 168:\n",
      "\t initialcontextdecoder_cost_cost: 96.8106536865\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 169\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 169:\n",
      "\t initialcontextdecoder_cost_cost: 133.019500732\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 170\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 170:\n",
      "\t initialcontextdecoder_cost_cost: 50.7088813782\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 171\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 171:\n",
      "\t initialcontextdecoder_cost_cost: 53.1025390625\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 172\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 172:\n",
      "\t initialcontextdecoder_cost_cost: 57.9300422668\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 173\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 173:\n",
      "\t initialcontextdecoder_cost_cost: 61.8450088501\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 174\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 174:\n",
      "\t initialcontextdecoder_cost_cost: 73.7310409546\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 175\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 175:\n",
      "\t initialcontextdecoder_cost_cost: 71.6534805298\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 176\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 176:\n",
      "\t initialcontextdecoder_cost_cost: 86.2908859253\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 177\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 177:\n",
      "\t initialcontextdecoder_cost_cost: 90.5139389038\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 178\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 178:\n",
      "\t initialcontextdecoder_cost_cost: 109.863647461\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 179\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 179:\n",
      "\t initialcontextdecoder_cost_cost: 135.992156982\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 180\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 180:\n",
      "\t initialcontextdecoder_cost_cost: 45.3237609863\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 181\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 181:\n",
      "\t initialcontextdecoder_cost_cost: 49.5747299194\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 182\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 182:\n",
      "\t initialcontextdecoder_cost_cost: 56.2838859558\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 183\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 183:\n",
      "\t initialcontextdecoder_cost_cost: 63.2949943542\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 184\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 184:\n",
      "\t initialcontextdecoder_cost_cost: 68.7234725952\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 185\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 185:\n",
      "\t initialcontextdecoder_cost_cost: 76.6927490234\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 186\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 186:\n",
      "\t initialcontextdecoder_cost_cost: 80.8088302612\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 187\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 187:\n",
      "\t initialcontextdecoder_cost_cost: 96.3158721924\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 188\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 188:\n",
      "\t initialcontextdecoder_cost_cost: 100.597427368\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 189\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 189:\n",
      "\t initialcontextdecoder_cost_cost: 139.136199951\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 190\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 190:\n",
      "\t initialcontextdecoder_cost_cost: 50.1831855774\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 191\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 191:\n",
      "\t initialcontextdecoder_cost_cost: 51.5952568054\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 192\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 192:\n",
      "\t initialcontextdecoder_cost_cost: 64.5074234009\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 193\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 193:\n",
      "\t initialcontextdecoder_cost_cost: 64.2812652588\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 194\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 194:\n",
      "\t initialcontextdecoder_cost_cost: 66.2378082275\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 195\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 195:\n",
      "\t initialcontextdecoder_cost_cost: 71.3598175049\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 196\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 196:\n",
      "\t initialcontextdecoder_cost_cost: 78.0453948975\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 197\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 197:\n",
      "\t initialcontextdecoder_cost_cost: 86.2434539795\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 198\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 198:\n",
      "\t initialcontextdecoder_cost_cost: 98.8306808472\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 199\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 199:\n",
      "\t initialcontextdecoder_cost_cost: 120.354690552\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 200\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 200:\n",
      "\t initialcontextdecoder_cost_cost: 41.5917549133\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 201\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 201:\n",
      "\t initialcontextdecoder_cost_cost: 50.6978225708\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 202\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 202:\n",
      "\t initialcontextdecoder_cost_cost: 56.0909538269\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 203\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 203:\n",
      "\t initialcontextdecoder_cost_cost: 66.5964202881\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 204\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 204:\n",
      "\t initialcontextdecoder_cost_cost: 65.6031036377\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 205\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 205:\n",
      "\t initialcontextdecoder_cost_cost: 81.0117340088\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 206\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 206:\n",
      "\t initialcontextdecoder_cost_cost: 84.9609222412\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 207\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 207:\n",
      "\t initialcontextdecoder_cost_cost: 93.8759765625\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 208\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 208:\n",
      "\t initialcontextdecoder_cost_cost: 103.263938904\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 209\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 209:\n",
      "\t initialcontextdecoder_cost_cost: 134.804962158\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 210\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 210:\n",
      "\t initialcontextdecoder_cost_cost: 43.0835990906\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 211\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 211:\n",
      "\t initialcontextdecoder_cost_cost: 50.0340881348\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 212\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 212:\n",
      "\t initialcontextdecoder_cost_cost: 56.908996582\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 213\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 213:\n",
      "\t initialcontextdecoder_cost_cost: 60.7933578491\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 214\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 214:\n",
      "\t initialcontextdecoder_cost_cost: 66.5556411743\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 215\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 215:\n",
      "\t initialcontextdecoder_cost_cost: 72.1386947632\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 216\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 216:\n",
      "\t initialcontextdecoder_cost_cost: 77.3242645264\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 217\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 217:\n",
      "\t initialcontextdecoder_cost_cost: 83.3291778564\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 218\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 218:\n",
      "\t initialcontextdecoder_cost_cost: 99.5794525146\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 219\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 219:\n",
      "\t initialcontextdecoder_cost_cost: 120.833595276\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 220\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 220:\n",
      "\t initialcontextdecoder_cost_cost: 47.2368927002\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 221\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 221:\n",
      "\t initialcontextdecoder_cost_cost: 49.0740509033\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 222\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 222:\n",
      "\t initialcontextdecoder_cost_cost: 54.2686386108\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 223\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 223:\n",
      "\t initialcontextdecoder_cost_cost: 57.1179389954\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 224\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 224:\n",
      "\t initialcontextdecoder_cost_cost: 71.2755966187\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 225\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 225:\n",
      "\t initialcontextdecoder_cost_cost: 72.5506820679\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 226\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 226:\n",
      "\t initialcontextdecoder_cost_cost: 79.7651824951\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 227\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 227:\n",
      "\t initialcontextdecoder_cost_cost: 86.1981582642\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 228\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 228:\n",
      "\t initialcontextdecoder_cost_cost: 104.021751404\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 229\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 229:\n",
      "\t initialcontextdecoder_cost_cost: 125.282730103\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 230\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 230:\n",
      "\t initialcontextdecoder_cost_cost: 53.3506851196\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 231\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 231:\n",
      "\t initialcontextdecoder_cost_cost: 47.1640548706\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 232\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 232:\n",
      "\t initialcontextdecoder_cost_cost: 53.4029846191\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 233\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 233:\n",
      "\t initialcontextdecoder_cost_cost: 57.6986808777\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 234\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 234:\n",
      "\t initialcontextdecoder_cost_cost: 70.7834472656\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 235\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 235:\n",
      "\t initialcontextdecoder_cost_cost: 70.1307525635\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 236\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 236:\n",
      "\t initialcontextdecoder_cost_cost: 81.0791015625\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 237\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 237:\n",
      "\t initialcontextdecoder_cost_cost: 89.7954559326\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 238\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 238:\n",
      "\t initialcontextdecoder_cost_cost: 100.427032471\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 239\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 239:\n",
      "\t initialcontextdecoder_cost_cost: 127.707130432\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 240\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 240:\n",
      "\t initialcontextdecoder_cost_cost: 46.4939689636\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 241\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 241:\n",
      "\t initialcontextdecoder_cost_cost: 48.1097183228\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 242\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 242:\n",
      "\t initialcontextdecoder_cost_cost: 55.0183792114\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 243\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 243:\n",
      "\t initialcontextdecoder_cost_cost: 59.5987663269\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 244\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 244:\n",
      "\t initialcontextdecoder_cost_cost: 66.9251403809\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 245\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 245:\n",
      "\t initialcontextdecoder_cost_cost: 73.7966461182\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 246\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 246:\n",
      "\t initialcontextdecoder_cost_cost: 77.3060836792\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 247\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 247:\n",
      "\t initialcontextdecoder_cost_cost: 82.2936706543\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 248\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 248:\n",
      "\t initialcontextdecoder_cost_cost: 98.4768447876\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 249\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 249:\n",
      "\t initialcontextdecoder_cost_cost: 129.225524902\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 250\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 250:\n",
      "\t initialcontextdecoder_cost_cost: 46.6244049072\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 251\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 251:\n",
      "\t initialcontextdecoder_cost_cost: 48.5383071899\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 252\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 252:\n",
      "\t initialcontextdecoder_cost_cost: 58.5636711121\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 253\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 253:\n",
      "\t initialcontextdecoder_cost_cost: 57.4515075684\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 254\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 254:\n",
      "\t initialcontextdecoder_cost_cost: 67.8653564453\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 255\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 255:\n",
      "\t initialcontextdecoder_cost_cost: 69.3911895752\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 256\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 256:\n",
      "\t initialcontextdecoder_cost_cost: 78.7370758057\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 257\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 257:\n",
      "\t initialcontextdecoder_cost_cost: 85.16847229\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 258\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 258:\n",
      "\t initialcontextdecoder_cost_cost: 99.1403045654\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 259\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 259:\n",
      "\t initialcontextdecoder_cost_cost: 122.142433167\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 260\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 260:\n",
      "\t initialcontextdecoder_cost_cost: 45.0095596313\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 261\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 261:\n",
      "\t initialcontextdecoder_cost_cost: 49.4072494507\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 262\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 262:\n",
      "\t initialcontextdecoder_cost_cost: 58.3912353516\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 263\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 263:\n",
      "\t initialcontextdecoder_cost_cost: 61.2789993286\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 264\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 264:\n",
      "\t initialcontextdecoder_cost_cost: 65.607421875\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 265\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 265:\n",
      "\t initialcontextdecoder_cost_cost: 71.9546890259\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 266\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 266:\n",
      "\t initialcontextdecoder_cost_cost: 79.3477783203\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 267\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 267:\n",
      "\t initialcontextdecoder_cost_cost: 86.0847396851\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 268\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 268:\n",
      "\t initialcontextdecoder_cost_cost: 95.3187713623\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 269\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 269:\n",
      "\t initialcontextdecoder_cost_cost: 117.620254517\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 270\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 270:\n",
      "\t initialcontextdecoder_cost_cost: 47.4580764771\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 271\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 271:\n",
      "\t initialcontextdecoder_cost_cost: 47.3346252441\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 272\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 272:\n",
      "\t initialcontextdecoder_cost_cost: 56.1423950195\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 273\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 273:\n",
      "\t initialcontextdecoder_cost_cost: 56.5850753784\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 274\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 274:\n",
      "\t initialcontextdecoder_cost_cost: 62.7682380676\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 275\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 275:\n",
      "\t initialcontextdecoder_cost_cost: 63.5200424194\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 276\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 276:\n",
      "\t initialcontextdecoder_cost_cost: 79.5832061768\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 277\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 277:\n",
      "\t initialcontextdecoder_cost_cost: 84.5547332764\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 278\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 278:\n",
      "\t initialcontextdecoder_cost_cost: 96.0926132202\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 279\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 279:\n",
      "\t initialcontextdecoder_cost_cost: 125.128738403\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 280\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 280:\n",
      "\t initialcontextdecoder_cost_cost: 46.2365036011\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 281\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 281:\n",
      "\t initialcontextdecoder_cost_cost: 49.4194946289\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 282\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 282:\n",
      "\t initialcontextdecoder_cost_cost: 54.6417617798\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 283\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 283:\n",
      "\t initialcontextdecoder_cost_cost: 57.2396316528\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 284\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 284:\n",
      "\t initialcontextdecoder_cost_cost: 68.200088501\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 285\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 285:\n",
      "\t initialcontextdecoder_cost_cost: 67.6931152344\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 286\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 286:\n",
      "\t initialcontextdecoder_cost_cost: 80.8777923584\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 287\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 287:\n",
      "\t initialcontextdecoder_cost_cost: 81.5750274658\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 288\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 288:\n",
      "\t initialcontextdecoder_cost_cost: 101.973220825\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 289\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 289:\n",
      "\t initialcontextdecoder_cost_cost: 124.188430786\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 290\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 290:\n",
      "\t initialcontextdecoder_cost_cost: 48.6216316223\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 291\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 291:\n",
      "\t initialcontextdecoder_cost_cost: 48.9422225952\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 292\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 292:\n",
      "\t initialcontextdecoder_cost_cost: 58.4558334351\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 293\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 293:\n",
      "\t initialcontextdecoder_cost_cost: 59.6267700195\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 294\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 294:\n",
      "\t initialcontextdecoder_cost_cost: 65.3478546143\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 295\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 295:\n",
      "\t initialcontextdecoder_cost_cost: 67.6828918457\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 296\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 296:\n",
      "\t initialcontextdecoder_cost_cost: 73.9720687866\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 297\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 297:\n",
      "\t initialcontextdecoder_cost_cost: 83.1113510132\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 298\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 298:\n",
      "\t initialcontextdecoder_cost_cost: 100.645751953\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 299\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 299:\n",
      "\t initialcontextdecoder_cost_cost: 133.452728271\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 300\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 300:\n",
      "\t initialcontextdecoder_cost_cost: 44.27085495\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 301\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 301:\n",
      "\t initialcontextdecoder_cost_cost: 47.3357810974\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 302\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 302:\n",
      "\t initialcontextdecoder_cost_cost: 53.7530288696\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 303\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 303:\n",
      "\t initialcontextdecoder_cost_cost: 55.8167190552\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 304\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 304:\n",
      "\t initialcontextdecoder_cost_cost: 65.6107711792\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 305\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 305:\n",
      "\t initialcontextdecoder_cost_cost: 66.876449585\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 306\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 306:\n",
      "\t initialcontextdecoder_cost_cost: 76.9544525146\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 307\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 307:\n",
      "\t initialcontextdecoder_cost_cost: 81.8983459473\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 308\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 308:\n",
      "\t initialcontextdecoder_cost_cost: 95.2601165771\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 309\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 309:\n",
      "\t initialcontextdecoder_cost_cost: 121.343978882\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 310\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 310:\n",
      "\t initialcontextdecoder_cost_cost: 45.1272087097\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(exp_config, train_stream, dev_stream, use_bokeh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OLD CODE AFTER THIS CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(encoder, decoder):\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    source_sentence = tensor.lmatrix('source')\n",
    "    source_sentence_mask = tensor.matrix('source_mask')\n",
    "    \n",
    "    \n",
    "\n",
    "    initial_context = tensor.matrix('initial_context')\n",
    "    \n",
    "    representation, source_sentence_mask,\n",
    "             target_sentence, target_sentence_mask, initial_state_context\n",
    "\n",
    "    # the name is important to make sure pre-trained params get loaded correctly\n",
    "    # decoder.name = 'decoder'\n",
    "\n",
    "    # This is the part that is different for the MinimumRiskSequenceGenerator\n",
    "    cost = initial_context_decoder.cost(\n",
    "        encoder.apply(source_sentence, source_sentence_mask),\n",
    "        source_sentence_mask, samples, samples_mask, scores)\n",
    "\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = next(train_image_stream.get_epoch_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(x[0])\n",
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theano_sample_func = sample_model.get_theano_function()\n",
    "\n",
    "# close over the sampling func and the trg_vocab to standardize the interface\n",
    "# TODO: actually this should be a callable class with params (sampling_func, trg_vocab)\n",
    "# TODO: we may be able to make this function faster by passing multiple sources for sampling at the same damn time\n",
    "# TODO: or by avoiding the for loop somehow\n",
    "def sampling_func(source_seq, num_samples=1):\n",
    "\n",
    "    def _get_true_length(seqs, vocab):\n",
    "        try:\n",
    "            lens = []\n",
    "            for r in seqs.tolist():\n",
    "                lens.append(r.index(vocab['</S>']) + 1)\n",
    "            return lens\n",
    "        except ValueError:\n",
    "            return [seqs.shape[1] for _ in range(seqs.shape[0])]\n",
    "\n",
    "    # samples = []\n",
    "    # for _ in range(num_samples):\n",
    "        # outputs of self.sampling_fn = outputs of sequence_generator.generate: next_states + [next_outputs] +\n",
    "        #                 list(next_glimpses.values()) + [next_costs])\n",
    "        # _1, outputs, _2, _3, costs = theano_sample_func(source_seq[None, :])\n",
    "        # if we are generating a single sample, the length of the output will be len(source_seq)*2\n",
    "        # see decoder.generate\n",
    "        # the output is a [seq_len, 1] array\n",
    "        # outputs = outputs.reshape(outputs.shape[0])\n",
    "        # outputs = outputs[:_get_true_length(outputs, trg_vocab)]\n",
    "        # samples.append(outputs)\n",
    "\n",
    "    inputs = numpy.tile(source_seq[None, :], (num_samples, 1))\n",
    "    # the output is [seq_len, batch]\n",
    "    _1, outputs, _2, _3, costs = theano_sample_func(inputs)\n",
    "    outputs = outputs.T\n",
    "\n",
    "    # TODO: this step could be avoided by computing the samples mask in a different way\n",
    "    lens = _get_true_length(outputs, trg_vocab)\n",
    "    samples = [s[:l] for s,l in zip(outputs.tolist(), lens)]\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "src_stream = get_textfile_stream(source_file=exp_config['src_data'], src_vocab=exp_config['src_vocab'],\n",
    "                                         src_vocab_size=exp_config['src_vocab_size'])\n",
    "\n",
    "# test_source_stream.sources = ('sources',)\n",
    "trg_stream = get_textfile_stream(source_file=exp_config['trg_data'], src_vocab=exp_config['trg_vocab'],\n",
    "                                         src_vocab_size=exp_config['trg_vocab_size'])\n",
    "\n",
    "# Merge them to get a source, target pair\n",
    "training_stream = Merge([src_stream,\n",
    "                         trg_stream],\n",
    "                         ('source', 'target'))\n",
    "\n",
    "# Filter sequences that are too long\n",
    "training_stream = Filter(training_stream,\n",
    "                         predicate=_too_long(seq_len=exp_config['seq_len']))\n",
    "\n",
    "# sampling_transformer = MTSampleStreamTransformer(sampling_func, fake_score, num_samples=5)\n",
    "sampling_transformer = MTSampleStreamTransformer(sampling_func, sentence_level_bleu, num_samples=exp_config['n_samples'])\n",
    "\n",
    "training_stream = Mapping(training_stream, sampling_transformer, add_sources=('samples', 'scores'))\n",
    "\n",
    "\n",
    "class FlattenSamples(Transformer):\n",
    "    \"\"\"Adds padding to variable-length sequences.\n",
    "\n",
    "    When your batches consist of variable-length sequences, use this class\n",
    "    to equalize lengths by adding zero-padding. To distinguish between\n",
    "    data and padding masks can be produced. For each data source that is\n",
    "    masked, a new source will be added. This source will have the name of\n",
    "    the original source with the suffix ``_mask`` (e.g. ``features_mask``).\n",
    "\n",
    "    Elements of incoming batches will be treated as numpy arrays (i.e.\n",
    "    using `numpy.asarray`). If they have more than one dimension,\n",
    "    all dimensions except length, that is the first one, must be equal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_stream : :class:`AbstractDataStream` instance\n",
    "        The data stream to wrap\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data_stream, **kwargs):\n",
    "        if data_stream.produces_examples:\n",
    "            raise ValueError('the wrapped data stream must produce batches of '\n",
    "                             'examples, not examples')\n",
    "        super(FlattenSamples, self).__init__(\n",
    "            data_stream, produces_examples=False, **kwargs)\n",
    "\n",
    "#         if mask_dtype is None:\n",
    "#             self.mask_dtype = config.floatX\n",
    "#         else:\n",
    "#             self.mask_dtype = mask_dtype\n",
    "\n",
    "    @property\n",
    "    def sources(self):\n",
    "        return self.data_stream.sources\n",
    "#         sources = []\n",
    "#         for source in self.data_stream.sources:\n",
    "#             sources.append(source)\n",
    "#             if source in self.mask_sources:\n",
    "#                 sources.append(source + '_mask')\n",
    "#         return tuple(sources)\n",
    "\n",
    "    def transform_batch(self, batch):\n",
    "        batch_with_flattened_samples = []\n",
    "        for i, (source, source_batch) in enumerate(\n",
    "                zip(self.data_stream.sources, batch)):\n",
    "#             if source not in self.mask_sources:\n",
    "#                 batch_with_masks.append(source_batch)\n",
    "#                 continue\n",
    "            if source == 'samples':\n",
    "                flattened_samples = []\n",
    "                for ins in source_batch:\n",
    "                    for sample in ins:\n",
    "                        flattened_samples.append(sample)\n",
    "                batch_with_flattened_samples.append(flattened_samples)\n",
    "            else:\n",
    "                batch_with_flattened_samples.append(source_batch)\n",
    "\n",
    "        return tuple(batch_with_flattened_samples)\n",
    "\n",
    "\n",
    "class CopySourceNTimes(Transformer):\n",
    "    \"\"\"Duplicate the source N times to match the number of samples\n",
    "\n",
    "    We need this transformer because the attention model expects one source sequence for each\n",
    "    target sequence, but in the sampling case there are effectively (instances*sample_size) target sequences\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_stream : :class:`AbstractDataStream` instance\n",
    "        The data stream to wrap\n",
    "    n_samples : int -- the number of samples that were generated for each source sequence\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data_stream, n_samples=5, **kwargs):\n",
    "        if data_stream.produces_examples:\n",
    "            raise ValueError('the wrapped data stream must produce batches of '\n",
    "                             'examples, not examples')\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "        super(CopySourceNTimes, self).__init__(\n",
    "            data_stream, produces_examples=False, **kwargs)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def sources(self):\n",
    "        return self.data_stream.sources\n",
    "\n",
    "    def transform_batch(self, batch):\n",
    "        batch_with_expanded_source = []\n",
    "        for i, (source, source_batch) in enumerate(\n",
    "                zip(self.data_stream.sources, batch)):\n",
    "            if source == 'source':\n",
    "#                 copy each source seqoyuence self.n_samples times, but keep the tensor 2d\n",
    "\n",
    "                expanded_source = []\n",
    "                for ins in source_batch:\n",
    "                    expanded_source.extend([ins for _ in range(self.n_samples)])\n",
    "\n",
    "                batch_with_expanded_source.append(expanded_source)\n",
    "            else:\n",
    "                batch_with_expanded_source.append(source_batch)\n",
    "\n",
    "        return tuple(batch_with_expanded_source)\n",
    "\n",
    "\n",
    "\n",
    "# Replace out of vocabulary tokens with unk token\n",
    "# training_stream = Mapping(training_stream,\n",
    "#                  _oov_to_unk(src_vocab_size=exp_config['src_vocab_size'],\n",
    "#                              trg_vocab_size=exp_config['trg_vocab_size'],\n",
    "#                              unk_id=exp_config['unk_id']))\n",
    "\n",
    "# Build a batched version of stream to read k batches ahead\n",
    "training_stream = Batch(training_stream,\n",
    "               iteration_scheme=ConstantScheme(\n",
    "                   exp_config['batch_size']*exp_config['sort_k_batches']))\n",
    "\n",
    "# Sort all samples in the read-ahead batch\n",
    "training_stream = Mapping(training_stream, SortMapping(_length))\n",
    "\n",
    "# Convert it into a stream again\n",
    "training_stream = Unpack(training_stream)\n",
    "\n",
    "# Construct batches from the stream with specified batch size\n",
    "training_stream = Batch(\n",
    "    training_stream, iteration_scheme=ConstantScheme(exp_config['batch_size']))\n",
    "\n",
    "# Pad sequences that are short\n",
    "# IDEA: add a transformer which flattens the target samples before we add the mask\n",
    "flat_sample_stream = FlattenSamples(training_stream)\n",
    "\n",
    "expanded_source_stream = CopySourceNTimes(flat_sample_stream, n_samples=exp_config['n_samples'])\n",
    "\n",
    "# TODO: some sources can be excluded from the padding Op, but since blocks matches sources with input variable\n",
    "# TODO: names, it's not critical\n",
    "masked_stream = PaddingWithEOS(\n",
    "    expanded_source_stream, [exp_config['src_vocab_size'] - 1, exp_config['trg_vocab_size'] - 1])\n",
    "\n",
    "\n",
    "def create_model(encoder, decoder):\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    source_sentence = tensor.lmatrix('source')\n",
    "    source_sentence_mask = tensor.matrix('source_mask')\n",
    "\n",
    "#     target_samples = tensor.tensor3('samples').astype('int64')\n",
    "#     target_samples_mask = tensor.tensor3('target_samples_mask').astype('int64')\n",
    "    samples = tensor.lmatrix('samples')\n",
    "    samples_mask = tensor.matrix('samples_mask')\n",
    "\n",
    "    # scores is (batch, samples)\n",
    "    scores = tensor.matrix('scores')\n",
    "    # We don't need a scores mask because there should be the same number of scores for each instance\n",
    "    # num samples is a hyperparameter of the model\n",
    "\n",
    "    # the name is important to make sure pre-trained params get loaded correctly\n",
    "#     decoder.name = 'decoder'\n",
    "\n",
    "    # This is the part that is different for the MinimumRiskSequenceGenerator\n",
    "    cost = decoder.expected_cost(\n",
    "        encoder.apply(source_sentence, source_sentence_mask),\n",
    "        source_sentence_mask, samples, samples_mask, scores)\n",
    "\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}